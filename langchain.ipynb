{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=TextLoader(\"D:\\Langchain_Project\\Data Ingestion\\speech.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content=\"Thank you for gathering today to discuss an issue of paramount importanceâ€”an issue that not only defines our generation but also determines the fate of future generations: global warming. This phenomenon, often referred to as climate change, has become one of the most pressing challenges of our time. Its impact is pervasive, affecting every corner of the globe, and its consequences are profound, threatening the very fabric of life on Earth.\\nTo fully grasp the gravity of global warming, we must first understand what it entails. Global warming refers to the long-term increase in Earth's average surface temperature due to human activities, primarily the emission of greenhouse gases such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O). These gases trap heat in the Earth's atmosphere, leading to what is commonly known as the greenhouse effect.\")]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents=loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Thank you for gathering today to discuss an issue of paramount importanceâ€”an issue that not only defines our generation but also determines the fate of future generations: global warming. This phenomenon, often referred to as climate change, has become one of the most pressing challenges of our time. Its impact is pervasive, affecting every corner of the globe, and its consequences are profound, threatening the very fabric of life on Earth.\\nTo fully grasp the gravity of global warming, we must first understand what it entails. Global warming refers to the long-term increase in Earth's average surface temperature due to human activities, primarily the emission of greenhouse gases such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O). These gases trap heat in the Earth's atmosphere, leading to what is commonly known as the greenhouse effect.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader(\"D:\\Langchain_Project\\Data Ingestion\\Karan Mehta Cover Letter.pdf\")\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\Karan Mehta Cover Letter.pdf', 'page': 0}, page_content='Karan Mehta  \\nApartment 14, James Plunket House  \\nDublin 8, D08TD80  \\n+353 0894690894  \\nkmehta883@gmail.com  \\nGitHub  | LinkedIn  \\nDear Three Ireland , \\nI am writing to express my enthusiasm for the Data Scientist position at your organization . With \\nextensive experience in AI/ML solutions, predictive analytics, and MLOps, I am eager to bring \\nmy expertise to Three Ireland . My background in developing and deploying advanced machine \\nlearning models aligns well with the innovative projects . \\nAs a Data Scientist at Edubridge India Pvt. Ltd., I led the development of a scalable data pipeline \\nthat significantly enhanced forecasting accuracy for a retail client, achieving a MAPE  (Mean \\nAbsolute Percentage Error)  of 10%. I collaborated with cross -functional teams to produce  \\nmachine learning models on AWS, leveraging tools such as Keras, TensorFlow, and sklearn. My \\nwork also included optimizing deep neural networks for banking clients using advanced \\ntechniques like Batch Normalization, Dropouts, and E arly Stopping, which improved model \\nefficiency and performance.  \\nAt All Soft Services and Solutions, I spearheaded a project for a transportation company, \\noptimizing fares and predicting demand, which increased operational efficiency. My role \\ninvolved extensive exploratory data analysis and feature engineering, ensuring  robust and \\naccurate model predictions. Additionally, my experience as a Data Science Instructor allowed me \\nto mentor students in data science and machine learning, further refining my ability to \\ncommunicate complex concepts effectively.  \\nMy technical proficiency spans Python, MySQL, PowerBi, AWS, and deep learning frameworks. \\nI am skilled in data cleaning, transformation, and visualization, and have a strong foundation in \\nregression analysis, statistical concepts, and analytical reporting.  My ability to manage end -to-\\nend projects, from data ingestion and preprocessing to model deployment and performance \\nevaluation, aligns with the responsibilities of a Data Scientist at any prestigious organization . \\nNotable projects during my academic and professional journey include:  \\n• Automated Video Surveillance System  with Email attachment system : Developed \\nusing with YOLO v8 transformer model, achieving a 76% accuracy rate in anomaly \\ndetection.  \\n• Speech Emotion Recognition : Implemented a CNN model to classify speech into seven \\nemotion classes, utilizing MFCC features and spectrograms.  \\n• End-to-End Insurance Premium Prediction System : Leveraged MLOps techniques \\nand AWS integration to improve risk assessment and pricing strategies, achieving a \\nMAPE of 1.6%.  '),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\Karan Mehta Cover Letter.pdf', 'page': 1}, page_content=\"My master's degree in Artificial Intelligence from Dublin Business School and a Bachelor of \\nComputer Applications from Sikkim Manipal University have provided a strong academic \\nfoundation. Additionally, certifications in Machine Learning and Deep Learning from IBM and \\nDatabricks underscore my commitment to continuous learning and staying abreast of the latest \\nadvancements in the field.  \\nI am confident that my skills and experience  make me a strong fit for the Data Scientist role, and \\nI am excited about the opportunity to contribute to Three Ireland . \\nSincerely,  \\nKaran Mehta  \\n \")]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.land/virtual-environments/virtualenv'}, page_content='Follow the courseWhy you need virtual environmentsPreventing version conflictsEasy to reproduce and installWorks everywhere, even when not administrator (root)Virtual environments vs. other optionsHow to create a Python venvPython 3.4 and aboveAll other Python versionsPython venv activationWindows venv activationLinux and MacOS venv activationHow a Python venv worksSo what does this PATH variable do? What’s inside a venv?Deactivate the Python venvDeleting a Python venvDelete a venv created with Virtualenv or python -m venvDelete a venv with PipenvDelete a venv with PoetryFollow the courseLearn moreConclusionGet certified with our courses')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader=WebBaseLoader(web_paths=(\"https://python.land/virtual-environments/virtualenv\",),\n",
    "                     bs_kwargs=dict(\n",
    "                         parse_only=bs4.SoupStrainer(\n",
    "                             class_=(\"entry_title\",\"entry_content\",\"wp-block-heading\"))))\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Follow the courseWhy you need virtual environmentsPreventing version conflictsEasy to reproduce and installWorks everywhere, even when not administrator (root)Virtual environments vs. other optionsHow to create a Python venvPython 3.4 and aboveAll other Python versionsPython venv activationWindows venv activationLinux and MacOS venv activationHow a Python venv worksSo what does this PATH variable do? What’s inside a venv?Deactivate the Python venvDeleting a Python venvDelete a venv created with Virtualenv or python -m venvDelete a venv with PipenvDelete a venv with PoetryFollow the courseLearn moreConclusionGet certified with our courses'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://www.espn.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_multiple_pages = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://www.espn.com/', 'title': 'ESPN - Serving Sports Fans. Anytime. Anywhere.', 'description': 'Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\nESPN - Serving Sports Fans. Anytime. Anywhere.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Skip to main content\\n    \\n\\n        Skip to navigation\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<\\n\\n>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenuESPN\\n\\n\\n\\n\\n\\nscores\\n\\n\\n\\nNFLNBAMLBNCAAFNHLSoccer…WNBABoxingCFLNCAACricketF1GolfHorseLLWSMMANASCARNBA G LeagueNBA Summer LeagueNCAAMNCAAWNWSLOlympicsPLLProfessional WrestlingRacingRN BBRN FBRugbySports BettingTennisX GamesUFLFantasyWatchESPN BETESPN+\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n\\nSubscribe Now\\n\\n\\n\\n\\n\\nFight Night: Cannonier vs. Borralho\\n\\n\\n\\n\\n\\n\\n\\nNCAA Football\\n\\n\\n\\n\\n\\n\\n\\nPGA TOUR LIVE: FedEx Cup Playoffs\\n\\n\\n\\n\\n\\n\\n\\nBundesliga\\n\\n\\n\\n\\n\\n\\n\\nLALIGA\\n\\n\\n\\n\\n\\n\\n\\nMLB\\n\\n\\nQuick Links\\n\\n\\n\\n\\nLittle League World Series\\n\\n\\n\\n\\n\\n\\n\\nMLB Standings\\n\\n\\n\\n\\n\\n\\n\\n2024 US Open\\n\\n\\n\\n\\n\\n\\n\\n2024 NFL Schedule\\n\\n\\n\\n\\n\\n\\n\\nWNBA Rookie Tracker\\n\\n\\n\\n\\n\\n\\n\\nSign up: Fantasy Football\\n\\n\\n\\n\\n\\n\\n\\nESPN Radio: Listen Live\\n\\n\\n\\n\\n\\n\\n\\nWatch Tennis on ESPN\\n\\n\\n\\n\\n\\n\\n\\nWatch Golf on ESPN\\n\\n\\n\\n\\n\\n\\nFavorites\\n\\n\\n\\n\\n\\n\\n      Manage Favorites\\n      \\n\\n\\n\\nCustomize ESPNCreate AccountLog InFantasy\\n\\n\\n\\n\\nFootball\\n\\n\\n\\n\\n\\n\\n\\nBaseball\\n\\n\\n\\n\\n\\n\\n\\nBasketball\\n\\n\\n\\n\\n\\n\\n\\nHockey\\n\\n\\nESPN Sites\\n\\n\\n\\n\\nESPN Deportes\\n\\n\\n\\n\\n\\n\\n\\nAndscape\\n\\n\\n\\n\\n\\n\\n\\nespnW\\n\\n\\n\\n\\n\\n\\n\\nESPNFC\\n\\n\\n\\n\\n\\n\\n\\nX Games\\n\\n\\n\\n\\n\\n\\n\\nSEC Network\\n\\n\\nESPN Apps\\n\\n\\n\\n\\nESPN\\n\\n\\n\\n\\n\\n\\n\\nESPN Fantasy\\n\\n\\n\\n\\n\\n\\n\\nTournament Challenge\\n\\n\\nFollow ESPN\\n\\n\\n\\n\\nFacebook\\n\\n\\n\\n\\n\\n\\n\\nX/Twitter\\n\\n\\n\\n\\n\\n\\n\\nInstagram\\n\\n\\n\\n\\n\\n\\n\\nSnapchat\\n\\n\\n\\n\\n\\n\\n\\nTikTok\\n\\n\\n\\n\\n\\n\\n\\nYouTube\\n\\n\\nSchefter's cheat sheet: Value picks, sleepers to targetNo one is more plugged in to the latest buzz around the NFL than Adam Schefter. We asked him to share his favorite values in fantasy drafts with you.16hAdam SchefterESPN2024 Fantasy Football Draft Guide: Rankings, mock drafts and analysisHere is our best content as you prep for draft season, including rankings, projections and analysis.2dFantasy StaffBowen's favorite players to target in 2024Sign up now for Fantasy FootballTOP HEADLINESCommanders finalize plans to build Taylor statueCanales hails Young's brief outing: 'Hell of a day'Tomlin lauds Wilson's 'results' in Steelers' lossHarbaugh lauds Herbert, others stuck in elevatorFlorida wins U.S. title, faces Taiwan in LLWS finalPulisic: Pochettino move 'good news' for USMNTDjokovic urges 'clear protocols' after Sinner rulingWilliams, top-rated OLB in '25, commits to TexasPreseason report cards for every rookieCOLLEGE FOOTBALL IS BACK!SATURDAY'S LATER GAMESSee AllRanking the top 10 senior prospects for the 2025 NFL draftAer Lingus College Football ClassicGeorgia Tech stuns No. 10 FSU with winning FG as time expires8h0:32Irish stout: How Georgia Tech dominated FSU up-front and secured a program-defining winThe Yellow Jackets used toughness at the line of scrimmage to shock Florida State.2hPete Thamel\\n\\n\\n\\nFSU marches down the field to tie it in the 4th quarter8h0:41\\n\\n\\nJamal Haynes fights his way into the end zone10h0:37\\n\\n\\nFSU's Edwin Joseph lays the boom on special teams9h0:27\\n\\n\\nRyan Fitzgerald drills a 59-yard FG to pull FSU even10h0:33\\n\\n\\nGeorgia Tech comes up with a big 4th down stop11h0:24\\n\\n\\nMalik Rutherford's 42-yard gain sets up a Georgia Tech TD10h0:46\\n\\n\\nLawrance Toafili breaks free for a 28-yard FSU TD10h0:25\\n\\n\\n\\nDublin is upset city: Georgia Tech stuns No. 10 FSU'College football is back': Trae Young, Jaylen Ramsey react to upsetNFL PRESEASON SCORESSATURDAY'S GAMESSee AllBEST OF THE PRESEASONJohn Jiles hurdles defender for a Giants first down33m0:19Simi Fehoko sheds a defender en route to a 78-yard TD catch3h0:27GETTING READY FOR THE SEASONNFL preseason Week 3 takeaways, 53-man roster predictions: RB Deuce Vaughn makes strong case for CowboysWeek 3 of the preseason will help determine teams' final 53-man rosters.28mNFL NationPhoto by Sam Hodde/Getty ImagesPreseason progress reports for every single rookie selected in the 2024 NFL draftCaleb Williams to Chop Robinson and all points in between, here is how every draft pick has fared so far.2dNFL Nation reportersUFC Fight Night - Middleweight - Main EventJared Cannonier vs. Caio BorralholiveExpert picks and best bets for UFC Fight Night, including two TUF finalesWho has the edge this week at UFC Fight Night? MMA coaches and betting insiders make their picks.14hMLB SCOREBOARDSATURDAY'S GAMESSee AllMUST-SEE FROM THE DIAMONDBowden Francis gets standing ovation after losing no-no in 9th5h0:40 Top HeadlinesCommanders finalize plans to build Taylor statueCanales hails Young's brief outing: 'Hell of a day'Tomlin lauds Wilson's 'results' in Steelers' lossHarbaugh lauds Herbert, others stuck in elevatorFlorida wins U.S. title, faces Taiwan in LLWS finalPulisic: Pochettino move 'good news' for USMNTDjokovic urges 'clear protocols' after Sinner rulingWilliams, top-rated OLB in '25, commits to TexasPreseason report cards for every rookieFavorites FantasyManage FavoritesFantasy HomeCustomize ESPNCreate AccountLog InICYMI0:25McIlroy bounces his 3-wood into the water after errant tee shotRory McIlroy is frustrated with his tee shot and flings his club into the water. Best of ESPN+ESPNNBA predictions 2024-25: Picks for East and West champs plus NBA FinalsWho in the East can stop the Celtics? Will the Nuggets climb back into Western Conference contention? Our panel reveals its championship predictions.ESPN2025 NFL mock draft: Yates' early first-round predictionsWith college football kicking off, let's project the first round of the 2025 draft. Could we see three QBs taken? Will NFL teams pursue defense early?ESPNCollege Football Playoff 2024: Preseason predictionsHeather Dinich picks the CFP field of 12 AND the eight teams on the bubble, then ranks the best of the Group of 5.Quinn Harris/Getty ImagesJuan Soto ... and who else? Early 2024 MLB free agency rankingsWhich players will rule the winter? It's time to take a look at who could get paid big this offseason. Trending NowIllustration by EnisaurusCollege Football Playoff PredictorSelect your favorite contender, make your picks for how its season will play out and see what the Allstate Playoff Predictor foretells.Bruce Bennett/Getty ImagesHow eight WNBA teams can make a championship run in 2024Eight teams will advance to the WNBA playoffs. Each of them is capable of a championship run. Yes, even the Fever and Sky.Illustration by ESPN2024 Fantasy Football Draft Guide: Rankings, mock drafts and analysisYour one-stop shop for our best content as you prep for draft season, including rankings, projections and analysis.Geoff Burke-USA TODAY SportsCould six-inning minimum starting pitcher rule come to MLB?Could one big rule change make starters stars again and inject offense into the game? Sign up to play the #1 Fantasy game!Create A LeagueJoin Public LeagueReactivate A LeagueMock Draft NowSign up for FREE!Create A LeagueJoin a Public LeagueReactivate a LeaguePractice With a Mock DraftSign up for FREE!Create A LeagueJoin a Public LeagueReactivate a LeaguePractice with a Mock DraftGet a custom ESPN experienceEnjoy the benefits of a personalized accountSelect your favorite leagues, teams and players and get the latest scores, news and updates that matter most to you. \\n\\nESPN+\\n\\n\\n\\n\\nFight Night: Cannonier vs. Borralho\\n\\n\\n\\n\\n\\n\\n\\nNCAA Football\\n\\n\\n\\n\\n\\n\\n\\nPGA TOUR LIVE: FedEx Cup Playoffs\\n\\n\\n\\n\\n\\n\\n\\nMLB\\n\\n\\nQuick Links\\n\\n\\n\\n\\nLittle League World Series\\n\\n\\n\\n\\n\\n\\n\\nMLB Standings\\n\\n\\n\\n\\n\\n\\n\\n2024 US Open\\n\\n\\n\\n\\n\\n\\n\\n2024 NFL Schedule\\n\\n\\n\\n\\n\\n\\n\\nWNBA Rookie Tracker\\n\\n\\n\\n\\n\\n\\n\\nSign up: Fantasy Football\\n\\n\\n\\n\\n\\n\\n\\nESPN Radio: Listen Live\\n\\n\\n\\n\\n\\n\\n\\nWatch Tennis on ESPN\\n\\n\\n\\n\\n\\n\\n\\nWatch Golf on ESPN\\n\\n\\nFantasy\\n\\n\\n\\n\\nFootball\\n\\n\\n\\n\\n\\n\\n\\nBaseball\\n\\n\\n\\n\\n\\n\\n\\nBasketball\\n\\n\\n\\n\\n\\n\\n\\nHockey\\n\\n\\nESPN Sites\\n\\n\\n\\n\\nESPN Deportes\\n\\n\\n\\n\\n\\n\\n\\nAndscape\\n\\n\\n\\n\\n\\n\\n\\nespnW\\n\\n\\n\\n\\n\\n\\n\\nESPNFC\\n\\n\\n\\n\\n\\n\\n\\nX Games\\n\\n\\n\\n\\n\\n\\n\\nSEC Network\\n\\n\\nESPN Apps\\n\\n\\n\\n\\nESPN\\n\\n\\n\\n\\n\\n\\n\\nESPN Fantasy\\n\\n\\n\\n\\n\\n\\n\\nTournament Challenge\\n\\n\\nFollow ESPN\\n\\n\\n\\n\\nFacebook\\n\\n\\n\\n\\n\\n\\n\\nX/Twitter\\n\\n\\n\\n\\n\\n\\n\\nInstagram\\n\\n\\n\\n\\n\\n\\n\\nSnapchat\\n\\n\\n\\n\\n\\n\\n\\nTikTok\\n\\n\\n\\n\\n\\n\\n\\nYouTube\\n\\n\\nTerms of UsePrivacy PolicyInterest-Based AdsEU Privacy RightsCookie PolicyManage Privacy Preferences© ESPN Enterprises, Inc. All rights reserved.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://www.espn.com/', 'title': 'ESPN - Serving Sports Fans. Anytime. Anywhere.', 'description': 'Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.', 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\Karan Mehta Cover Letter.pdf', 'page': 0}, page_content='Karan Mehta  \\nApartment 14, James Plunket House  \\nDublin 8, D08TD80  \\n+353 0894690894  \\nkmehta883@gmail.com  \\nGitHub  | LinkedIn  \\nDear Three Ireland , \\nI am writing to express my enthusiasm for the Data Scientist position at your organization . With \\nextensive experience in AI/ML solutions, predictive analytics, and MLOps, I am eager to bring \\nmy expertise to Three Ireland . My background in developing and deploying advanced machine \\nlearning models aligns well with the innovative projects . \\nAs a Data Scientist at Edubridge India Pvt. Ltd., I led the development of a scalable data pipeline \\nthat significantly enhanced forecasting accuracy for a retail client, achieving a MAPE  (Mean \\nAbsolute Percentage Error)  of 10%. I collaborated with cross -functional teams to produce  \\nmachine learning models on AWS, leveraging tools such as Keras, TensorFlow, and sklearn. My \\nwork also included optimizing deep neural networks for banking clients using advanced \\ntechniques like Batch Normalization, Dropouts, and E arly Stopping, which improved model \\nefficiency and performance.  \\nAt All Soft Services and Solutions, I spearheaded a project for a transportation company, \\noptimizing fares and predicting demand, which increased operational efficiency. My role \\ninvolved extensive exploratory data analysis and feature engineering, ensuring  robust and \\naccurate model predictions. Additionally, my experience as a Data Science Instructor allowed me \\nto mentor students in data science and machine learning, further refining my ability to \\ncommunicate complex concepts effectively.  \\nMy technical proficiency spans Python, MySQL, PowerBi, AWS, and deep learning frameworks. \\nI am skilled in data cleaning, transformation, and visualization, and have a strong foundation in \\nregression analysis, statistical concepts, and analytical reporting.  My ability to manage end -to-\\nend projects, from data ingestion and preprocessing to model deployment and performance \\nevaluation, aligns with the responsibilities of a Data Scientist at any prestigious organization . \\nNotable projects during my academic and professional journey include:  \\n• Automated Video Surveillance System  with Email attachment system : Developed \\nusing with YOLO v8 transformer model, achieving a 76% accuracy rate in anomaly \\ndetection.  \\n• Speech Emotion Recognition : Implemented a CNN model to classify speech into seven \\nemotion classes, utilizing MFCC features and spectrograms.  \\n• End-to-End Insurance Premium Prediction System : Leveraged MLOps techniques \\nand AWS integration to improve risk assessment and pricing strategies, achieving a \\nMAPE of 1.6%.  ')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting PDF to text\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\Karan Mehta Cover Letter.pdf', 'page': 0}, page_content='Karan Mehta  \\nApartment 14, James Plunket House  \\nDublin 8, D08TD80  \\n+353 0894690894  \\nkmehta883@gmail.com  \\nGitHub  | LinkedIn  \\nDear Three Ireland , \\nI am writing to express my enthusiasm for the Data Scientist position at your organization . With \\nextensive experience in AI/ML solutions, predictive analytics, and MLOps, I am eager to bring \\nmy expertise to Three Ireland . My background in developing and deploying advanced machine \\nlearning models aligns well with the innovative projects .'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\Karan Mehta Cover Letter.pdf', 'page': 0}, page_content='As a Data Scientist at Edubridge India Pvt. Ltd., I led the development of a scalable data pipeline \\nthat significantly enhanced forecasting accuracy for a retail client, achieving a MAPE  (Mean \\nAbsolute Percentage Error)  of 10%. I collaborated with cross -functional teams to produce  \\nmachine learning models on AWS, leveraging tools such as Keras, TensorFlow, and sklearn. My \\nwork also included optimizing deep neural networks for banking clients using advanced'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\Karan Mehta Cover Letter.pdf', 'page': 0}, page_content='techniques like Batch Normalization, Dropouts, and E arly Stopping, which improved model \\nefficiency and performance.  \\nAt All Soft Services and Solutions, I spearheaded a project for a transportation company, \\noptimizing fares and predicting demand, which increased operational efficiency. My role \\ninvolved extensive exploratory data analysis and feature engineering, ensuring  robust and \\naccurate model predictions. Additionally, my experience as a Data Science Instructor allowed me'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\Karan Mehta Cover Letter.pdf', 'page': 0}, page_content='to mentor students in data science and machine learning, further refining my ability to \\ncommunicate complex concepts effectively.  \\nMy technical proficiency spans Python, MySQL, PowerBi, AWS, and deep learning frameworks. \\nI am skilled in data cleaning, transformation, and visualization, and have a strong foundation in \\nregression analysis, statistical concepts, and analytical reporting.  My ability to manage end -to-'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\Karan Mehta Cover Letter.pdf', 'page': 0}, page_content='end projects, from data ingestion and preprocessing to model deployment and performance \\nevaluation, aligns with the responsibilities of a Data Scientist at any prestigious organization . \\nNotable projects during my academic and professional journey include:  \\n• Automated Video Surveillance System  with Email attachment system : Developed \\nusing with YOLO v8 transformer model, achieving a 76% accuracy rate in anomaly \\ndetection.'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\Karan Mehta Cover Letter.pdf', 'page': 0}, page_content='detection.  \\n• Speech Emotion Recognition : Implemented a CNN model to classify speech into seven \\nemotion classes, utilizing MFCC features and spectrograms.  \\n• End-to-End Insurance Premium Prediction System : Leveraged MLOps techniques \\nand AWS integration to improve risk assessment and pricing strategies, achieving a \\nMAPE of 1.6%.'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\Karan Mehta Cover Letter.pdf', 'page': 1}, page_content=\"My master's degree in Artificial Intelligence from Dublin Business School and a Bachelor of \\nComputer Applications from Sikkim Manipal University have provided a strong academic \\nfoundation. Additionally, certifications in Machine Learning and Deep Learning from IBM and \\nDatabricks underscore my commitment to continuous learning and staying abreast of the latest \\nadvancements in the field.  \\nI am confident that my skills and experience  make me a strong fit for the Data Scientist role, and\"),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\Karan Mehta Cover Letter.pdf', 'page': 1}, page_content='I am excited about the opportunity to contribute to Three Ireland . \\nSincerely,  \\nKaran Mehta')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "final_documents=text_splitter.split_documents(docs)\n",
    "final_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'techniques like Batch Normalization, Dropouts, and E arly Stopping, which improved model \\nefficiency and performance.  \\nAt All Soft Services and Solutions, I spearheaded a project for a transportation company, \\noptimizing fares and predicting demand, which increased operational efficiency. My role \\ninvolved extensive exploratory data analysis and feature engineering, ensuring  robust and \\naccurate model predictions. Additionally, my experience as a Data Science Instructor allowed me'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_documents[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Thank you for gathering today to discuss an issue'),\n",
       " Document(page_content='to discuss an issue of paramount importanceâ€”an'),\n",
       " Document(page_content='importanceâ€”an issue that not only defines our'),\n",
       " Document(page_content='only defines our generation but also determines'),\n",
       " Document(page_content='but also determines the fate of future'),\n",
       " Document(page_content='the fate of future generations: global warming.'),\n",
       " Document(page_content='global warming. This phenomenon, often referred'),\n",
       " Document(page_content='often referred to as climate change, has become'),\n",
       " Document(page_content='change, has become one of the most pressing'),\n",
       " Document(page_content='the most pressing challenges of our time. Its'),\n",
       " Document(page_content='of our time. Its impact is pervasive, affecting'),\n",
       " Document(page_content='affecting every corner of the globe, and its'),\n",
       " Document(page_content='the globe, and its consequences are profound,'),\n",
       " Document(page_content='are profound, threatening the very fabric of life'),\n",
       " Document(page_content='very fabric of life on Earth.'),\n",
       " Document(page_content='To fully grasp the gravity of global warming, we'),\n",
       " Document(page_content='global warming, we must first understand what it'),\n",
       " Document(page_content='understand what it entails. Global warming refers'),\n",
       " Document(page_content='warming refers to the long-term increase in'),\n",
       " Document(page_content=\"increase in Earth's average surface temperature\"),\n",
       " Document(page_content='surface temperature due to human activities,'),\n",
       " Document(page_content='human activities, primarily the emission of'),\n",
       " Document(page_content='the emission of greenhouse gases such as carbon'),\n",
       " Document(page_content='such as carbon dioxide (CO2), methane (CH4), and'),\n",
       " Document(page_content='methane (CH4), and nitrous oxide (N2O). These'),\n",
       " Document(page_content=\"oxide (N2O). These gases trap heat in the Earth's\"),\n",
       " Document(page_content=\"heat in the Earth's atmosphere, leading to what\"),\n",
       " Document(page_content='leading to what is commonly known as the'),\n",
       " Document(page_content='known as the greenhouse effect.')]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech=\"\"\n",
    "with open('D:\\Langchain_Project\\Data Ingestion\\speech.txt') as f:\n",
    "    speech=f.read()\n",
    "\n",
    "speech\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=50,chunk_overlap=20)\n",
    "final_text=text_splitter.create_documents([speech])\n",
    "final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\nâ€¦\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between usâ€”however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendshipâ€”exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our heartsâ€”for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader('D:\\Langchain_Project\\Data Ingestion\\speech.txt')\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 470, which is longer than the specified 150\n",
      "Created a chunk of size 347, which is longer than the specified 150\n",
      "Created a chunk of size 670, which is longer than the specified 150\n",
      "Created a chunk of size 984, which is longer than the specified 150\n",
      "Created a chunk of size 791, which is longer than the specified 150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='Just because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='â€¦'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='It will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between usâ€”however hard it may be for them, for the time being, to believe that this is spoken from our hearts.'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='We have borne with their present government through all these bitter months because of that friendshipâ€”exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='It is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our heartsâ€”for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='To such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_splitter=CharacterTextSplitter(separator=\"\\n\\n\",chunk_size=150,chunk_overlap=20)\n",
    "text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 470, which is longer than the specified 100\n",
      "Created a chunk of size 347, which is longer than the specified 100\n",
      "Created a chunk of size 670, which is longer than the specified 100\n",
      "Created a chunk of size 984, which is longer than the specified 100\n",
      "Created a chunk of size 791, which is longer than the specified 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='Just because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech=\"\"\n",
    "with open('D:\\Langchain_Project\\Data Ingestion\\speech.txt') as f:\n",
    "    speech=f.read()\n",
    "\n",
    "text_splitter=CharacterTextSplitter(chunk_size=100,chunk_overlap=20)\n",
    "text=text_splitter.create_documents([speech])\n",
    "text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Skip to main content  \\nLangChain 0.2 is out! Leave feedback on the v0.2 docs here. You can view the v0.1 docs here.  \\nIntegrations  \\nAPI reference  \\nLatestLegacy  \\nMore  \\nPeopleContributingCookbooks3rd party tutorialsYouTubearXiv  \\n💬  \\nv0.2  \\nv0.2v0.1  \\n🦜️🔗  \\nLangSmithLangSmith DocsLangChain HubJS/TS Docs  \\nSearch  \\nIntroductionConceptual guideSecurity  \\nTutorials  \\nBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize Text  \\nHow-to guides  \\nHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector stores  \\nEcosystem  \\n🦜🛠️ LangSmith🦜🕸️ LangGraph  \\nVersions  \\nOverview of v0.2Release policyPydantic compatibility  \\nMigrating to v0.2  \\nMigrating to LangChain v0.2astream_events v2Changes  \\nMigrating from v0.0 chains  \\nHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChain  \\nHow-to guidesHow to add chat history  \\nOn this page  \\nHow to add chat historySetup\\u200bDependencies\\u200bLangSmith\\u200bChains\\u200bLLM\\u200bRetriever\\u200bPrompt\\u200bAssembling the chain\\u200bAdding chat history\\u200bTying it together\\u200bAgents\\u200bRetrieval tool\\u200bAgent constructor\\u200bTying it together\\u200bNext steps\\u200b'),\n",
       " Document(metadata={'Header 1': 'How to add chat history'}, page_content='In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.  \\nIn this guide we focus on adding logic for incorporating historical messages.  \\nThis is largely a condensed version of the Conversational RAG tutorial.  \\nWe will cover two approaches:  \\nChains, in which we always execute a retrieval step;Agents, in which we give an LLM discretion over whether and how to execute a retrieval step (or multiple steps).  \\nFor the external knowledge source, we will use the same LLM Powered Autonomous Agents blog post by Lilian Weng from the RAG tutorial.'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Setup\\u200b', 'Header 3': 'Dependencies\\u200b'}, page_content='We\\'ll use OpenAI embeddings and a Chroma vector store in this walkthrough, but everything shown here works with any Embeddings, and VectorStore or Retriever.  \\nWe\\'ll use the following packages:  \\n%%capture --no-stderr%pip install --upgrade --quiet langchain langchain-community langchain-chroma beautifulsoup4  \\nWe need to set environment variable OPENAI_API_KEY, which can be done directly or loaded from a .env file like so:  \\nimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"): os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()# import dotenv# dotenv.load_dotenv()'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Setup\\u200b', 'Header 3': 'LangSmith\\u200b'}, page_content='Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.  \\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:  \\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"if not os.environ.get(\"LANGCHAIN_API_KEY\"): os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Chains\\u200b'}, page_content='In a conversational RAG application, queries issued to the retriever should be informed by the context of the conversation. LangChain provides a create_history_aware_retriever constructor to simplify this. It constructs a chain that accepts keys input and chat_history as input, and has the same output schema as a retriever. create_history_aware_retriever requires as inputs:  \\nLLM;Retriever;Prompt.  \\nFirst we obtain these objects:'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Chains\\u200b', 'Header 3': 'LLM\\u200b'}, page_content='We can use any supported chat model:  \\nOpenAIAnthropicAzureGoogleCohereNVIDIAFireworksAIGroqMistralAITogetherAI  \\npip install -qU langchain-openai  \\nimport getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\")  \\npip install -qU langchain-anthropic  \\nimport getpassimport osos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()from langchain_anthropic import ChatAnthropicllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")  \\npip install -qU langchain-openai  \\nimport getpassimport osos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import AzureChatOpenAIllm = AzureChatOpenAI( azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"], azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"], openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],)  \\npip install -qU langchain-google-vertexai  \\nimport getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from langchain_google_vertexai import ChatVertexAIllm = ChatVertexAI(model=\"gemini-1.5-flash\")  \\npip install -qU langchain-cohere  \\nimport getpassimport osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere import ChatCoherellm = ChatCohere(model=\"command-r-plus\")  \\npip install -qU langchain-nvidia-ai-endpoints  \\nimport getpassimport osos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()from langchain import ChatNVIDIAllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")  \\npip install -qU langchain-fireworks  \\nimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()from langchain_fireworks import ChatFireworksllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")  \\npip install -qU langchain-groq  \\nimport getpassimport osos.environ[\"GROQ_API_KEY\"] = getpass.getpass()from langchain_groq import ChatGroqllm = ChatGroq(model=\"llama3-8b-8192\")  \\npip install -qU langchain-mistralai  \\nimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()from langchain_mistralai import ChatMistralAIllm = ChatMistralAI(model=\"mistral-large-latest\")  \\npip install -qU langchain-openai  \\nimport getpassimport osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI( base_url=\"https://api.together.xyz/v1\", api_key=os.environ[\"TOGETHER_API_KEY\"], model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Chains\\u200b', 'Header 3': 'Retriever\\u200b'}, page_content='For the retriever, we will use WebBaseLoader to load the content of a web page. Here we instantiate a Chroma vectorstore and then use its .as_retriever method to build a retriever that can be incorporated into LCEL chains.  \\nimport bs4from langchain.chains import create_retrieval_chainfrom langchain.chains.combine_documents import create_stuff_documents_chainfrom langchain_chroma import Chromafrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitterloader = WebBaseLoader( web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(\"post-content\", \"post-title\", \"post-header\") ) ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)splits = text_splitter.split_documents(docs)vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()  \\nAPI Reference:create_retrieval_chain | create_stuff_documents_chain | WebBaseLoader | StrOutputParser | ChatPromptTemplate | RunnablePassthrough | OpenAIEmbeddings | RecursiveCharacterTextSplitter'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Chains\\u200b', 'Header 3': 'Prompt\\u200b'}, page_content='We\\'ll use a prompt that includes a MessagesPlaceholder variable under the name \"chat_history\". This allows us to pass in a list of Messages to the prompt using the \"chat_history\" input key, and these messages will be inserted after the system message and before the human message containing the latest question.  \\nfrom langchain.chains import create_history_aware_retrieverfrom langchain_core.prompts import MessagesPlaceholdercontextualize_q_system_prompt = ( \"Given a chat history and the latest user question \" \"which might reference context in the chat history, \" \"formulate a standalone question which can be understood \" \"without the chat history. Do NOT answer the question, \" \"just reformulate it if needed and otherwise return it as is.\")contextualize_q_prompt = ChatPromptTemplate.from_messages( [ (\"system\", contextualize_q_system_prompt), MessagesPlaceholder(\"chat_history\"), (\"human\", \"{input}\"), ])  \\nAPI Reference:create_history_aware_retriever | MessagesPlaceholder'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Chains\\u200b', 'Header 3': 'Assembling the chain\\u200b'}, page_content='We can then instantiate the history-aware retriever:  \\nhistory_aware_retriever = create_history_aware_retriever( llm, retriever, contextualize_q_prompt)  \\nThis chain prepends a rephrasing of the input query to our retriever, so that the retrieval incorporates the context of the conversation.  \\nNow we can build our full QA chain.  \\nAs in the RAG tutorial, we will use create_stuff_documents_chain to generate a question_answer_chain, with input keys context, chat_history, and input-- it accepts the retrieved context alongside the conversation history and query to generate an answer.  \\nWe build our final rag_chain with create_retrieval_chain. This chain applies the history_aware_retriever and question_answer_chain in sequence, retaining intermediate outputs such as the retrieved context for convenience. It has input keys input and chat_history, and includes input, chat_history, context, and answer in its output.  \\nsystem_prompt = ( \"You are an assistant for question-answering tasks. \" \"Use the following pieces of retrieved context to answer \" \"the question. If you don\\'t know the answer, say that you \" \"don\\'t know. Use three sentences maximum and keep the \" \"answer concise.\" \"\\\\n\\\\n\" \"{context}\")qa_prompt = ChatPromptTemplate.from_messages( [ (\"system\", system_prompt), MessagesPlaceholder(\"chat_history\"), (\"human\", \"{input}\"), ])question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Chains\\u200b', 'Header 3': 'Adding chat history\\u200b'}, page_content='To manage the chat history, we will need:  \\nAn object for storing the chat history;An object that wraps our chain and manages updates to the chat history.  \\nFor these we will use BaseChatMessageHistory and RunnableWithMessageHistory. The latter is a wrapper for an LCEL chain and a BaseChatMessageHistory that handles injecting chat history into inputs and updating it after each invocation.  \\nFor a detailed walkthrough of how to use these classes together to create a stateful conversational chain, head to the How to add message history (memory) LCEL how-to guide.  \\nBelow, we implement a simple example of the second option, in which chat histories are stored in a simple dict. LangChain manages memory integrations with Redis and other technologies to provide for more robust persistence.  \\nInstances of RunnableWithMessageHistory manage the chat history for you. They accept a config with a key (\"session_id\" by default) that specifies what conversation history to fetch and prepend to the input, and append the output to the same conversation history. Below is an example:  \\nfrom langchain_community.chat_message_histories import ChatMessageHistoryfrom langchain_core.chat_history import BaseChatMessageHistoryfrom langchain_core.runnables.history import RunnableWithMessageHistorystore = {}def get_session_history(session_id: str) -> BaseChatMessageHistory: if session_id not in store: store[session_id] = ChatMessageHistory() return store[session_id]conversational_rag_chain = RunnableWithMessageHistory( rag_chain, get_session_history, input_messages_key=\"input\", history_messages_key=\"chat_history\", output_messages_key=\"answer\",)  \\nAPI Reference:ChatMessageHistory | BaseChatMessageHistory | RunnableWithMessageHistory  \\nconversational_rag_chain.invoke( {\"input\": \"What is Task Decomposition?\"}, config={ \"configurable\": {\"session_id\": \"abc123\"} }, # constructs a key \"abc123\" in `store`.)[\"answer\"]  \\n\\'Task decomposition involves breaking down a complex task into smaller and simpler steps to make it more manageable and easier to accomplish. This process can be done using techniques like Chain of Thought (CoT) or Tree of Thoughts to guide the model in breaking down tasks effectively. Task decomposition can be facilitated by providing simple prompts to a language model, task-specific instructions, or human inputs.\\'  \\nconversational_rag_chain.invoke( {\"input\": \"What are common ways of doing it?\"}, config={\"configurable\": {\"session_id\": \"abc123\"}},)[\"answer\"]  \\n\\'Task decomposition can be achieved through various methods, including using techniques like Chain of Thought (CoT) or Tree of Thoughts to guide the model in breaking down tasks effectively. Common ways of task decomposition include providing simple prompts to a language model, task-specific instructions, or human inputs to break down complex tasks into smaller and more manageable steps. Additionally, task decomposition can involve utilizing resources like internet access for information gathering, long-term memory management, and GPT-3.5 powered agents for delegation of simple tasks.\\'  \\nThe conversation history can be inspected in the store dict:  \\nfrom langchain_core.messages import AIMessagefor message in store[\"abc123\"].messages: if isinstance(message, AIMessage): prefix = \"AI\" else: prefix = \"User\" print(f\"{prefix}: {message.content}\\\\n\")  \\nAPI Reference:AIMessage  \\nUser: What is Task Decomposition?AI: Task decomposition involves breaking down a complex task into smaller and simpler steps to make it more manageable and easier to accomplish. This process can be done using techniques like Chain of Thought (CoT) or Tree of Thoughts to guide the model in breaking down tasks effectively. Task decomposition can be facilitated by providing simple prompts to a language model, task-specific instructions, or human inputs.User: What are common ways of doing it?AI: Task decomposition can be achieved through various methods, including using techniques like Chain of Thought (CoT) or Tree of Thoughts to guide the model in breaking down tasks effectively. Common ways of task decomposition include providing simple prompts to a language model, task-specific instructions, or human inputs to break down complex tasks into smaller and more manageable steps. Additionally, task decomposition can involve utilizing resources like internet access for information gathering, long-term memory management, and GPT-3.5 powered agents for delegation of simple tasks.'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Chains\\u200b', 'Header 3': 'Tying it together\\u200b'}, page_content='For convenience, we tie together all of the necessary steps in a single code cell:  \\nimport bs4from langchain.chains import create_history_aware_retriever, create_retrieval_chainfrom langchain.chains.combine_documents import create_stuff_documents_chainfrom langchain_chroma import Chromafrom langchain_community.chat_message_histories import ChatMessageHistoryfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.chat_history import BaseChatMessageHistoryfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_core.runnables.history import RunnableWithMessageHistoryfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitterllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)### Construct retriever ###loader = WebBaseLoader( web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(\"post-content\", \"post-title\", \"post-header\") ) ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)splits = text_splitter.split_documents(docs)vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()### Contextualize question ###contextualize_q_system_prompt = ( \"Given a chat history and the latest user question \" \"which might reference context in the chat history, \" \"formulate a standalone question which can be understood \" \"without the chat history. Do NOT answer the question, \" \"just reformulate it if needed and otherwise return it as is.\")contextualize_q_prompt = ChatPromptTemplate.from_messages( [ (\"system\", contextualize_q_system_prompt), MessagesPlaceholder(\"chat_history\"), (\"human\", \"{input}\"), ])history_aware_retriever = create_history_aware_retriever( llm, retriever, contextualize_q_prompt)### Answer question ###system_prompt = ( \"You are an assistant for question-answering tasks. \" \"Use the following pieces of retrieved context to answer \" \"the question. If you don\\'t know the answer, say that you \" \"don\\'t know. Use three sentences maximum and keep the \" \"answer concise.\" \"\\\\n\\\\n\" \"{context}\")qa_prompt = ChatPromptTemplate.from_messages( [ (\"system\", system_prompt), MessagesPlaceholder(\"chat_history\"), (\"human\", \"{input}\"), ])question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)### Statefully manage chat history ###store = {}def get_session_history(session_id: str) -> BaseChatMessageHistory: if session_id not in store: store[session_id] = ChatMessageHistory() return store[session_id]conversational_rag_chain = RunnableWithMessageHistory( rag_chain, get_session_history, input_messages_key=\"input\", history_messages_key=\"chat_history\", output_messages_key=\"answer\",)  \\nAPI Reference:create_history_aware_retriever | create_retrieval_chain | create_stuff_documents_chain | ChatMessageHistory | WebBaseLoader | BaseChatMessageHistory | ChatPromptTemplate | MessagesPlaceholder | RunnableWithMessageHistory | ChatOpenAI | OpenAIEmbeddings | RecursiveCharacterTextSplitter  \\nconversational_rag_chain.invoke( {\"input\": \"What is Task Decomposition?\"}, config={ \"configurable\": {\"session_id\": \"abc123\"} }, # constructs a key \"abc123\" in `store`.)[\"answer\"]  \\n\\'Task decomposition involves breaking down a complex task into smaller and simpler steps to make it more manageable. Techniques like Chain of Thought (CoT) and Tree of Thoughts help in decomposing hard tasks into multiple manageable tasks by instructing models to think step by step and explore multiple reasoning possibilities at each step. Task decomposition can be achieved through various methods such as using prompting techniques, task-specific instructions, or human inputs.\\'  \\nconversational_rag_chain.invoke( {\"input\": \"What are common ways of doing it?\"}, config={\"configurable\": {\"session_id\": \"abc123\"}},)[\"answer\"]  \\n\\'Task decomposition can be done in common ways such as using prompting techniques like Chain of Thought (CoT) or Tree of Thoughts, which instruct models to think step by step and explore multiple reasoning possibilities at each step. Another way is to provide task-specific instructions, such as asking to \"Write a story outline\" for writing a novel, to guide the decomposition process. Additionally, task decomposition can also involve human inputs to break down complex tasks into smaller and simpler steps.\\''),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Agents\\u200b'}, page_content='Agents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allow you to offload some discretion over the retrieval process. Although their behavior is less predictable than chains, they offer some advantages in this context:  \\nAgents generate the input to the retriever directly, without necessarily needing us to explicitly build in contextualization, as we did above;Agents can execute multiple retrieval steps in service of a query, or refrain from executing a retrieval step altogether (e.g., in response to a generic greeting from a user).'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Agents\\u200b', 'Header 3': 'Retrieval tool\\u200b'}, page_content='Agents can access \"tools\" and manage their execution. In this case, we will convert our retriever into a LangChain tool to be wielded by the agent:  \\nfrom langchain.tools.retriever import create_retriever_tooltool = create_retriever_tool( retriever, \"blog_post_retriever\", \"Searches and returns excerpts from the Autonomous Agents blog post.\",)tools = [tool]  \\nAPI Reference:create_retriever_tool'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Agents\\u200b', 'Header 3': 'Agent constructor\\u200b'}, page_content='Now that we have defined the tools and the LLM, we can create the agent. We will be using LangGraph to construct the agent. Currently we are using a high level interface to construct the agent, but the nice thing about LangGraph is that this high-level interface is backed by a low-level, highly controllable API in case you want to modify the agent logic.  \\nfrom langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(llm, tools)  \\nWe can now try it out. Note that so far it is not stateful (we still need to add in memory)  \\nfrom langchain_core.messages import HumanMessagequery = \"What is Task Decomposition?\"for s in agent_executor.stream( {\"messages\": [HumanMessage(content=query)]},): print(s) print(\"----\")  \\nAPI Reference:HumanMessage  \\nError in LangChainTracer.on_tool_end callback: TracerException(\"Found chain run at ID 5cd28d13-88dd-4eac-a465-3770ac27eff6, but expected {\\'tool\\'} run.\")``````output{\\'agent\\': {\\'messages\\': [AIMessage(content=\\'\\', additional_kwargs={\\'tool_calls\\': [{\\'id\\': \\'call_TbhPPPN05GKi36HLeaN4QM90\\', \\'function\\': {\\'arguments\\': \\'{\"query\":\"Task Decomposition\"}\\', \\'name\\': \\'blog_post_retriever\\'}, \\'type\\': \\'function\\'}]}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 19, \\'prompt_tokens\\': 68, \\'total_tokens\\': 87}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': None, \\'finish_reason\\': \\'tool_calls\\', \\'logprobs\\': None}, id=\\'run-2e60d910-879a-4a2a-b1e9-6a6c5c7d7ebc-0\\', tool_calls=[{\\'name\\': \\'blog_post_retriever\\', \\'args\\': {\\'query\\': \\'Task Decomposition\\'}, \\'id\\': \\'call_TbhPPPN05GKi36HLeaN4QM90\\'}])]}}----{\\'tools\\': {\\'messages\\': [ToolMessage(content=\\'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\\\n\\\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\\\n\\\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\\\n\\\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\', name=\\'blog_post_retriever\\', tool_call_id=\\'call_TbhPPPN05GKi36HLeaN4QM90\\')]}}----{\\'agent\\': {\\'messages\\': [AIMessage(content=\\'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. This approach helps in transforming big tasks into multiple manageable tasks, making it easier for autonomous agents to handle and interpret the thinking process. One common method for task decomposition is the Chain of Thought (CoT) technique, where models are instructed to \"think step by step\" to decompose hard tasks. Another extension of CoT is the Tree of Thoughts, which explores multiple reasoning possibilities at each step by creating a tree structure of multiple thoughts per step. Task decomposition can be facilitated through various methods such as using simple prompts, task-specific instructions, or human inputs.\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 130, \\'prompt_tokens\\': 636, \\'total_tokens\\': 766}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': None, \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-3ef17638-65df-4030-a7fe-795e6da91c69-0\\')]}}----  \\nLangGraph comes with built in persistence, so we don\\'t need to use ChatMessageHistory! Rather, we can pass in a checkpointer to our LangGraph agent directly.  \\nDistinct conversations are managed by specifying a key for a conversation thread in the config dict, as shown below.  \\nfrom langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()agent_executor = create_react_agent(llm, tools, checkpointer=memory)  \\nThis is all we need to construct a conversational RAG agent.  \\nLet\\'s observe its behavior. Note that if we input a query that does not require a retrieval step, the agent does not execute one:  \\nconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}for s in agent_executor.stream( {\"messages\": [HumanMessage(content=\"Hi! I\\'m bob\")]}, config=config): print(s) print(\"----\")  \\n{\\'agent\\': {\\'messages\\': [AIMessage(content=\\'Hello Bob! How can I assist you today?\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 11, \\'prompt_tokens\\': 67, \\'total_tokens\\': 78}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': None, \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-1cd17562-18aa-4839-b41b-403b17a0fc20-0\\')]}}----  \\nFurther, if we input a query that does require a retrieval step, the agent generates the input to the tool:  \\nquery = \"What is Task Decomposition?\"for s in agent_executor.stream( {\"messages\": [HumanMessage(content=query)]}, config=config): print(s) print(\"----\")  \\nError in LangChainTracer.on_tool_end callback: TracerException(\"Found chain run at ID c54381c0-c5d9-495a-91a0-aca4ae755663, but expected {\\'tool\\'} run.\")``````output{\\'agent\\': {\\'messages\\': [AIMessage(content=\\'\\', additional_kwargs={\\'tool_calls\\': [{\\'id\\': \\'call_rg7zKTE5e0ICxVSslJ1u9LMg\\', \\'function\\': {\\'arguments\\': \\'{\"query\":\"Task Decomposition\"}\\', \\'name\\': \\'blog_post_retriever\\'}, \\'type\\': \\'function\\'}]}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 19, \\'prompt_tokens\\': 91, \\'total_tokens\\': 110}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': None, \\'finish_reason\\': \\'tool_calls\\', \\'logprobs\\': None}, id=\\'run-122bf097-7ff1-49aa-b430-e362b51354ad-0\\', tool_calls=[{\\'name\\': \\'blog_post_retriever\\', \\'args\\': {\\'query\\': \\'Task Decomposition\\'}, \\'id\\': \\'call_rg7zKTE5e0ICxVSslJ1u9LMg\\'}])]}}----{\\'tools\\': {\\'messages\\': [ToolMessage(content=\\'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\\\n\\\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\\\n\\\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\\\n\\\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\', name=\\'blog_post_retriever\\', tool_call_id=\\'call_rg7zKTE5e0ICxVSslJ1u9LMg\\')]}}----{\\'agent\\': {\\'messages\\': [AIMessage(content=\\'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. This approach helps in managing and solving intricate problems by dividing them into more manageable components. By decomposing tasks, agents or models can better understand the steps involved and plan their actions accordingly. Techniques like Chain of Thought (CoT) and Tree of Thoughts are examples of methods that enhance model performance on complex tasks by breaking them down into smaller steps.\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 87, \\'prompt_tokens\\': 659, \\'total_tokens\\': 746}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': None, \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-b9166386-83e5-4b82-9a4b-590e5fa76671-0\\')]}}----  \\nAbove, instead of inserting our query verbatim into the tool, the agent stripped unnecessary words like \"what\" and \"is\".  \\nThis same principle allows the agent to use the context of the conversation when necessary:  \\nquery = \"What according to the blog post are common ways of doing it? redo the search\"for s in agent_executor.stream( {\"messages\": [HumanMessage(content=query)]}, config=config): print(s) print(\"----\")  \\n{\\'agent\\': {\\'messages\\': [AIMessage(content=\\'\\', additional_kwargs={\\'tool_calls\\': [{\\'id\\': \\'call_6kbxTU5CDWLmF9mrvR7bWSkI\\', \\'function\\': {\\'arguments\\': \\'{\"query\":\"Common ways of task decomposition\"}\\', \\'name\\': \\'blog_post_retriever\\'}, \\'type\\': \\'function\\'}]}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 21, \\'prompt_tokens\\': 769, \\'total_tokens\\': 790}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': None, \\'finish_reason\\': \\'tool_calls\\', \\'logprobs\\': None}, id=\\'run-2d2c8327-35cd-484a-b8fd-52436657c2d8-0\\', tool_calls=[{\\'name\\': \\'blog_post_retriever\\', \\'args\\': {\\'query\\': \\'Common ways of task decomposition\\'}, \\'id\\': \\'call_6kbxTU5CDWLmF9mrvR7bWSkI\\'}])]}}----``````outputError in LangChainTracer.on_tool_end callback: TracerException(\"Found chain run at ID 29553415-e0f4-41a9-8921-ba489e377f68, but expected {\\'tool\\'} run.\")``````output{\\'tools\\': {\\'messages\\': [ToolMessage(content=\\'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\\\n\\\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\\\n\\\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\\\n\\\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\', name=\\'blog_post_retriever\\', tool_call_id=\\'call_6kbxTU5CDWLmF9mrvR7bWSkI\\')]}}----{\\'agent\\': {\\'messages\\': [AIMessage(content=\\'Common ways of task decomposition include:\\\\n1. Using LLM with simple prompting like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\"\\\\n2. Using task-specific instructions, for example, \"Write a story outline\" for writing a novel.\\\\n3. Involving human inputs in the task decomposition process.\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 67, \\'prompt_tokens\\': 1339, \\'total_tokens\\': 1406}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': None, \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-9ad14cde-ca75-4238-a868-f865e0fc50dd-0\\')]}}----  \\nNote that the agent was able to infer that \"it\" in our query refers to \"task decomposition\", and generated a reasonable search query as a result-- in this case, \"common ways of task decomposition\".'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Agents\\u200b', 'Header 3': 'Tying it together\\u200b'}, page_content='For convenience, we tie together all of the necessary steps in a single code cell:  \\nimport bs4from langchain.tools.retriever import create_retriever_toolfrom langchain_chroma import Chromafrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)### Construct retriever ###loader = WebBaseLoader( web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(\"post-content\", \"post-title\", \"post-header\") ) ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)splits = text_splitter.split_documents(docs)vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()### Build retriever tool ###tool = create_retriever_tool( retriever, \"blog_post_retriever\", \"Searches and returns excerpts from the Autonomous Agents blog post.\",)tools = [tool]agent_executor = create_react_agent(llm, tools, checkpointer=memory)  \\nAPI Reference:create_retriever_tool | WebBaseLoader | ChatOpenAI | OpenAIEmbeddings | RecursiveCharacterTextSplitter'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Next steps\\u200b'}, page_content='We\\'ve covered the steps to build a basic conversational Q&A application:  \\nWe used chains to build a predictable application that generates search queries for each user input;We used agents to build an application that \"decides\" when and how to generate search queries.  \\nTo explore different types of retrievers and retrieval strategies, visit the retrievers section of the how-to guides.  \\nFor a detailed walkthrough of LangChain\\'s conversation memory abstractions, visit the How to add message history (memory) LCEL page.  \\nTo learn more about agents, head to the Agents Modules.'),\n",
       " Document(page_content='Edit this page  \\nWas this page helpful?You can also leave detailed feedback on GitHub.  \\nPrevious  \\nHow to use LangChain with different Pydantic versions  \\nNext  \\nHow to get a RAG application to add citations  \\nSetupChainsAgentsNext steps  \\nDependenciesLangSmith  \\nLLMRetrieverPromptAssembling the chainAdding chat historyTying it together  \\nRetrieval toolAgent constructorTying it together  \\nCommunity  \\nTwitter  \\nGitHub  \\nOrganizationPythonJS/TS  \\nMore  \\nHomepageBlogYouTube  \\nCopyright © 2024 LangChain, Inc.')]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "url=\"https://python.langchain.com/v0.2/docs/how_to/qa_chat_history_how_to/#tying-it-together-1\"\n",
    "header_to_split=[\n",
    "    (\"h1\",\"Header 1\"),\n",
    "    (\"h2\",\"Header 2\"),\n",
    "    (\"h3\",\"Header 3\"),\n",
    "    (\"h4\",\"Header 4\")\n",
    "]\n",
    "\n",
    "html_spliter=HTMLHeaderTextSplitter(header_to_split)\n",
    "html_header_splits=html_spliter.split_text_from_url(url)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Skip to main content  \\nLangChain 0.2 is out! Leave feedback on the v0.2 docs here. You can view the v0.1 docs here.  \\nIntegrations  \\nAPI reference  \\nLatestLegacy  \\nMore  \\nPeopleContributingCookbooks3rd party tutorialsYouTubearXiv  \\n💬  \\nv0.2  \\nv0.2v0.1  \\n🦜️🔗  \\nLangSmithLangSmith DocsLangChain HubJS/TS Docs  \\nSearch  \\nIntroductionConceptual guideSecurity  \\nTutorials  \\nBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize Text  \\nHow-to guides  \\nHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector stores  \\nEcosystem  \\n🦜🛠️ LangSmith🦜🕸️ LangGraph  \\nVersions  \\nOverview of v0.2Release policyPydantic compatibility  \\nMigrating to v0.2  \\nMigrating to LangChain v0.2astream_events v2Changes  \\nMigrating from v0.0 chains  \\nHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChain  \\nHow-to guidesHow to add chat history  \\nOn this page  \\nHow to add chat historySetup\\u200bDependencies\\u200bLangSmith\\u200bChains\\u200bLLM\\u200bRetriever\\u200bPrompt\\u200bAssembling the chain\\u200bAdding chat history\\u200bTying it together\\u200bAgents\\u200bRetrieval tool\\u200bAgent constructor\\u200bTying it together\\u200bNext steps\\u200b'),\n",
       " Document(metadata={'Header 1': 'How to add chat history'}, page_content='In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.  \\nIn this guide we focus on adding logic for incorporating historical messages.  \\nThis is largely a condensed version of the Conversational RAG tutorial.  \\nWe will cover two approaches:  \\nChains, in which we always execute a retrieval step;Agents, in which we give an LLM discretion over whether and how to execute a retrieval step (or multiple steps).  \\nFor the external knowledge source, we will use the same LLM Powered Autonomous Agents blog post by Lilian Weng from the RAG tutorial.'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Setup\\u200b', 'Header 3': 'Dependencies\\u200b'}, page_content='We\\'ll use OpenAI embeddings and a Chroma vector store in this walkthrough, but everything shown here works with any Embeddings, and VectorStore or Retriever.  \\nWe\\'ll use the following packages:  \\n%%capture --no-stderr%pip install --upgrade --quiet langchain langchain-community langchain-chroma beautifulsoup4  \\nWe need to set environment variable OPENAI_API_KEY, which can be done directly or loaded from a .env file like so:  \\nimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"): os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()# import dotenv# dotenv.load_dotenv()'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Setup\\u200b', 'Header 3': 'LangSmith\\u200b'}, page_content='Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.  \\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:  \\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"if not os.environ.get(\"LANGCHAIN_API_KEY\"): os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Chains\\u200b'}, page_content='In a conversational RAG application, queries issued to the retriever should be informed by the context of the conversation. LangChain provides a create_history_aware_retriever constructor to simplify this. It constructs a chain that accepts keys input and chat_history as input, and has the same output schema as a retriever. create_history_aware_retriever requires as inputs:  \\nLLM;Retriever;Prompt.  \\nFirst we obtain these objects:'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Chains\\u200b', 'Header 3': 'LLM\\u200b'}, page_content='We can use any supported chat model:  \\nOpenAIAnthropicAzureGoogleCohereNVIDIAFireworksAIGroqMistralAITogetherAI  \\npip install -qU langchain-openai  \\nimport getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\")  \\npip install -qU langchain-anthropic  \\nimport getpassimport osos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()from langchain_anthropic import ChatAnthropicllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")  \\npip install -qU langchain-openai  \\nimport getpassimport osos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import AzureChatOpenAIllm = AzureChatOpenAI( azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"], azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"], openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],)  \\npip install -qU langchain-google-vertexai  \\nimport getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from langchain_google_vertexai import ChatVertexAIllm = ChatVertexAI(model=\"gemini-1.5-flash\")  \\npip install -qU langchain-cohere  \\nimport getpassimport osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere import ChatCoherellm = ChatCohere(model=\"command-r-plus\")  \\npip install -qU langchain-nvidia-ai-endpoints  \\nimport getpassimport osos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()from langchain import ChatNVIDIAllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")  \\npip install -qU langchain-fireworks  \\nimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()from langchain_fireworks import ChatFireworksllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")  \\npip install -qU langchain-groq  \\nimport getpassimport osos.environ[\"GROQ_API_KEY\"] = getpass.getpass()from langchain_groq import ChatGroqllm = ChatGroq(model=\"llama3-8b-8192\")  \\npip install -qU langchain-mistralai  \\nimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()from langchain_mistralai import ChatMistralAIllm = ChatMistralAI(model=\"mistral-large-latest\")  \\npip install -qU langchain-openai  \\nimport getpassimport osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI( base_url=\"https://api.together.xyz/v1\", api_key=os.environ[\"TOGETHER_API_KEY\"], model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Chains\\u200b', 'Header 3': 'Retriever\\u200b'}, page_content='For the retriever, we will use WebBaseLoader to load the content of a web page. Here we instantiate a Chroma vectorstore and then use its .as_retriever method to build a retriever that can be incorporated into LCEL chains.  \\nimport bs4from langchain.chains import create_retrieval_chainfrom langchain.chains.combine_documents import create_stuff_documents_chainfrom langchain_chroma import Chromafrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitterloader = WebBaseLoader( web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(\"post-content\", \"post-title\", \"post-header\") ) ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)splits = text_splitter.split_documents(docs)vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()  \\nAPI Reference:create_retrieval_chain | create_stuff_documents_chain | WebBaseLoader | StrOutputParser | ChatPromptTemplate | RunnablePassthrough | OpenAIEmbeddings | RecursiveCharacterTextSplitter'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Chains\\u200b', 'Header 3': 'Prompt\\u200b'}, page_content='We\\'ll use a prompt that includes a MessagesPlaceholder variable under the name \"chat_history\". This allows us to pass in a list of Messages to the prompt using the \"chat_history\" input key, and these messages will be inserted after the system message and before the human message containing the latest question.  \\nfrom langchain.chains import create_history_aware_retrieverfrom langchain_core.prompts import MessagesPlaceholdercontextualize_q_system_prompt = ( \"Given a chat history and the latest user question \" \"which might reference context in the chat history, \" \"formulate a standalone question which can be understood \" \"without the chat history. Do NOT answer the question, \" \"just reformulate it if needed and otherwise return it as is.\")contextualize_q_prompt = ChatPromptTemplate.from_messages( [ (\"system\", contextualize_q_system_prompt), MessagesPlaceholder(\"chat_history\"), (\"human\", \"{input}\"), ])  \\nAPI Reference:create_history_aware_retriever | MessagesPlaceholder'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Chains\\u200b', 'Header 3': 'Assembling the chain\\u200b'}, page_content='We can then instantiate the history-aware retriever:  \\nhistory_aware_retriever = create_history_aware_retriever( llm, retriever, contextualize_q_prompt)  \\nThis chain prepends a rephrasing of the input query to our retriever, so that the retrieval incorporates the context of the conversation.  \\nNow we can build our full QA chain.  \\nAs in the RAG tutorial, we will use create_stuff_documents_chain to generate a question_answer_chain, with input keys context, chat_history, and input-- it accepts the retrieved context alongside the conversation history and query to generate an answer.  \\nWe build our final rag_chain with create_retrieval_chain. This chain applies the history_aware_retriever and question_answer_chain in sequence, retaining intermediate outputs such as the retrieved context for convenience. It has input keys input and chat_history, and includes input, chat_history, context, and answer in its output.  \\nsystem_prompt = ( \"You are an assistant for question-answering tasks. \" \"Use the following pieces of retrieved context to answer \" \"the question. If you don\\'t know the answer, say that you \" \"don\\'t know. Use three sentences maximum and keep the \" \"answer concise.\" \"\\\\n\\\\n\" \"{context}\")qa_prompt = ChatPromptTemplate.from_messages( [ (\"system\", system_prompt), MessagesPlaceholder(\"chat_history\"), (\"human\", \"{input}\"), ])question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Chains\\u200b', 'Header 3': 'Adding chat history\\u200b'}, page_content='To manage the chat history, we will need:  \\nAn object for storing the chat history;An object that wraps our chain and manages updates to the chat history.  \\nFor these we will use BaseChatMessageHistory and RunnableWithMessageHistory. The latter is a wrapper for an LCEL chain and a BaseChatMessageHistory that handles injecting chat history into inputs and updating it after each invocation.  \\nFor a detailed walkthrough of how to use these classes together to create a stateful conversational chain, head to the How to add message history (memory) LCEL how-to guide.  \\nBelow, we implement a simple example of the second option, in which chat histories are stored in a simple dict. LangChain manages memory integrations with Redis and other technologies to provide for more robust persistence.  \\nInstances of RunnableWithMessageHistory manage the chat history for you. They accept a config with a key (\"session_id\" by default) that specifies what conversation history to fetch and prepend to the input, and append the output to the same conversation history. Below is an example:  \\nfrom langchain_community.chat_message_histories import ChatMessageHistoryfrom langchain_core.chat_history import BaseChatMessageHistoryfrom langchain_core.runnables.history import RunnableWithMessageHistorystore = {}def get_session_history(session_id: str) -> BaseChatMessageHistory: if session_id not in store: store[session_id] = ChatMessageHistory() return store[session_id]conversational_rag_chain = RunnableWithMessageHistory( rag_chain, get_session_history, input_messages_key=\"input\", history_messages_key=\"chat_history\", output_messages_key=\"answer\",)  \\nAPI Reference:ChatMessageHistory | BaseChatMessageHistory | RunnableWithMessageHistory  \\nconversational_rag_chain.invoke( {\"input\": \"What is Task Decomposition?\"}, config={ \"configurable\": {\"session_id\": \"abc123\"} }, # constructs a key \"abc123\" in `store`.)[\"answer\"]  \\n\\'Task decomposition involves breaking down a complex task into smaller and simpler steps to make it more manageable and easier to accomplish. This process can be done using techniques like Chain of Thought (CoT) or Tree of Thoughts to guide the model in breaking down tasks effectively. Task decomposition can be facilitated by providing simple prompts to a language model, task-specific instructions, or human inputs.\\'  \\nconversational_rag_chain.invoke( {\"input\": \"What are common ways of doing it?\"}, config={\"configurable\": {\"session_id\": \"abc123\"}},)[\"answer\"]  \\n\\'Task decomposition can be achieved through various methods, including using techniques like Chain of Thought (CoT) or Tree of Thoughts to guide the model in breaking down tasks effectively. Common ways of task decomposition include providing simple prompts to a language model, task-specific instructions, or human inputs to break down complex tasks into smaller and more manageable steps. Additionally, task decomposition can involve utilizing resources like internet access for information gathering, long-term memory management, and GPT-3.5 powered agents for delegation of simple tasks.\\'  \\nThe conversation history can be inspected in the store dict:  \\nfrom langchain_core.messages import AIMessagefor message in store[\"abc123\"].messages: if isinstance(message, AIMessage): prefix = \"AI\" else: prefix = \"User\" print(f\"{prefix}: {message.content}\\\\n\")  \\nAPI Reference:AIMessage  \\nUser: What is Task Decomposition?AI: Task decomposition involves breaking down a complex task into smaller and simpler steps to make it more manageable and easier to accomplish. This process can be done using techniques like Chain of Thought (CoT) or Tree of Thoughts to guide the model in breaking down tasks effectively. Task decomposition can be facilitated by providing simple prompts to a language model, task-specific instructions, or human inputs.User: What are common ways of doing it?AI: Task decomposition can be achieved through various methods, including using techniques like Chain of Thought (CoT) or Tree of Thoughts to guide the model in breaking down tasks effectively. Common ways of task decomposition include providing simple prompts to a language model, task-specific instructions, or human inputs to break down complex tasks into smaller and more manageable steps. Additionally, task decomposition can involve utilizing resources like internet access for information gathering, long-term memory management, and GPT-3.5 powered agents for delegation of simple tasks.'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Chains\\u200b', 'Header 3': 'Tying it together\\u200b'}, page_content='For convenience, we tie together all of the necessary steps in a single code cell:  \\nimport bs4from langchain.chains import create_history_aware_retriever, create_retrieval_chainfrom langchain.chains.combine_documents import create_stuff_documents_chainfrom langchain_chroma import Chromafrom langchain_community.chat_message_histories import ChatMessageHistoryfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.chat_history import BaseChatMessageHistoryfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_core.runnables.history import RunnableWithMessageHistoryfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitterllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)### Construct retriever ###loader = WebBaseLoader( web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(\"post-content\", \"post-title\", \"post-header\") ) ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)splits = text_splitter.split_documents(docs)vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()### Contextualize question ###contextualize_q_system_prompt = ( \"Given a chat history and the latest user question \" \"which might reference context in the chat history, \" \"formulate a standalone question which can be understood \" \"without the chat history. Do NOT answer the question, \" \"just reformulate it if needed and otherwise return it as is.\")contextualize_q_prompt = ChatPromptTemplate.from_messages( [ (\"system\", contextualize_q_system_prompt), MessagesPlaceholder(\"chat_history\"), (\"human\", \"{input}\"), ])history_aware_retriever = create_history_aware_retriever( llm, retriever, contextualize_q_prompt)### Answer question ###system_prompt = ( \"You are an assistant for question-answering tasks. \" \"Use the following pieces of retrieved context to answer \" \"the question. If you don\\'t know the answer, say that you \" \"don\\'t know. Use three sentences maximum and keep the \" \"answer concise.\" \"\\\\n\\\\n\" \"{context}\")qa_prompt = ChatPromptTemplate.from_messages( [ (\"system\", system_prompt), MessagesPlaceholder(\"chat_history\"), (\"human\", \"{input}\"), ])question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)### Statefully manage chat history ###store = {}def get_session_history(session_id: str) -> BaseChatMessageHistory: if session_id not in store: store[session_id] = ChatMessageHistory() return store[session_id]conversational_rag_chain = RunnableWithMessageHistory( rag_chain, get_session_history, input_messages_key=\"input\", history_messages_key=\"chat_history\", output_messages_key=\"answer\",)  \\nAPI Reference:create_history_aware_retriever | create_retrieval_chain | create_stuff_documents_chain | ChatMessageHistory | WebBaseLoader | BaseChatMessageHistory | ChatPromptTemplate | MessagesPlaceholder | RunnableWithMessageHistory | ChatOpenAI | OpenAIEmbeddings | RecursiveCharacterTextSplitter  \\nconversational_rag_chain.invoke( {\"input\": \"What is Task Decomposition?\"}, config={ \"configurable\": {\"session_id\": \"abc123\"} }, # constructs a key \"abc123\" in `store`.)[\"answer\"]  \\n\\'Task decomposition involves breaking down a complex task into smaller and simpler steps to make it more manageable. Techniques like Chain of Thought (CoT) and Tree of Thoughts help in decomposing hard tasks into multiple manageable tasks by instructing models to think step by step and explore multiple reasoning possibilities at each step. Task decomposition can be achieved through various methods such as using prompting techniques, task-specific instructions, or human inputs.\\'  \\nconversational_rag_chain.invoke( {\"input\": \"What are common ways of doing it?\"}, config={\"configurable\": {\"session_id\": \"abc123\"}},)[\"answer\"]  \\n\\'Task decomposition can be done in common ways such as using prompting techniques like Chain of Thought (CoT) or Tree of Thoughts, which instruct models to think step by step and explore multiple reasoning possibilities at each step. Another way is to provide task-specific instructions, such as asking to \"Write a story outline\" for writing a novel, to guide the decomposition process. Additionally, task decomposition can also involve human inputs to break down complex tasks into smaller and simpler steps.\\''),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Agents\\u200b'}, page_content='Agents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allow you to offload some discretion over the retrieval process. Although their behavior is less predictable than chains, they offer some advantages in this context:  \\nAgents generate the input to the retriever directly, without necessarily needing us to explicitly build in contextualization, as we did above;Agents can execute multiple retrieval steps in service of a query, or refrain from executing a retrieval step altogether (e.g., in response to a generic greeting from a user).'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Agents\\u200b', 'Header 3': 'Retrieval tool\\u200b'}, page_content='Agents can access \"tools\" and manage their execution. In this case, we will convert our retriever into a LangChain tool to be wielded by the agent:  \\nfrom langchain.tools.retriever import create_retriever_tooltool = create_retriever_tool( retriever, \"blog_post_retriever\", \"Searches and returns excerpts from the Autonomous Agents blog post.\",)tools = [tool]  \\nAPI Reference:create_retriever_tool'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Agents\\u200b', 'Header 3': 'Agent constructor\\u200b'}, page_content='Now that we have defined the tools and the LLM, we can create the agent. We will be using LangGraph to construct the agent. Currently we are using a high level interface to construct the agent, but the nice thing about LangGraph is that this high-level interface is backed by a low-level, highly controllable API in case you want to modify the agent logic.  \\nfrom langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(llm, tools)  \\nWe can now try it out. Note that so far it is not stateful (we still need to add in memory)  \\nfrom langchain_core.messages import HumanMessagequery = \"What is Task Decomposition?\"for s in agent_executor.stream( {\"messages\": [HumanMessage(content=query)]},): print(s) print(\"----\")  \\nAPI Reference:HumanMessage  \\nError in LangChainTracer.on_tool_end callback: TracerException(\"Found chain run at ID 5cd28d13-88dd-4eac-a465-3770ac27eff6, but expected {\\'tool\\'} run.\")``````output{\\'agent\\': {\\'messages\\': [AIMessage(content=\\'\\', additional_kwargs={\\'tool_calls\\': [{\\'id\\': \\'call_TbhPPPN05GKi36HLeaN4QM90\\', \\'function\\': {\\'arguments\\': \\'{\"query\":\"Task Decomposition\"}\\', \\'name\\': \\'blog_post_retriever\\'}, \\'type\\': \\'function\\'}]}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 19, \\'prompt_tokens\\': 68, \\'total_tokens\\': 87}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': None, \\'finish_reason\\': \\'tool_calls\\', \\'logprobs\\': None}, id=\\'run-2e60d910-879a-4a2a-b1e9-6a6c5c7d7ebc-0\\', tool_calls=[{\\'name\\': \\'blog_post_retriever\\', \\'args\\': {\\'query\\': \\'Task Decomposition\\'}, \\'id\\': \\'call_TbhPPPN05GKi36HLeaN4QM90\\'}])]}}----{\\'tools\\': {\\'messages\\': [ToolMessage(content=\\'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\\\n\\\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\\\n\\\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\\\n\\\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\', name=\\'blog_post_retriever\\', tool_call_id=\\'call_TbhPPPN05GKi36HLeaN4QM90\\')]}}----{\\'agent\\': {\\'messages\\': [AIMessage(content=\\'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. This approach helps in transforming big tasks into multiple manageable tasks, making it easier for autonomous agents to handle and interpret the thinking process. One common method for task decomposition is the Chain of Thought (CoT) technique, where models are instructed to \"think step by step\" to decompose hard tasks. Another extension of CoT is the Tree of Thoughts, which explores multiple reasoning possibilities at each step by creating a tree structure of multiple thoughts per step. Task decomposition can be facilitated through various methods such as using simple prompts, task-specific instructions, or human inputs.\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 130, \\'prompt_tokens\\': 636, \\'total_tokens\\': 766}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': None, \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-3ef17638-65df-4030-a7fe-795e6da91c69-0\\')]}}----  \\nLangGraph comes with built in persistence, so we don\\'t need to use ChatMessageHistory! Rather, we can pass in a checkpointer to our LangGraph agent directly.  \\nDistinct conversations are managed by specifying a key for a conversation thread in the config dict, as shown below.  \\nfrom langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()agent_executor = create_react_agent(llm, tools, checkpointer=memory)  \\nThis is all we need to construct a conversational RAG agent.  \\nLet\\'s observe its behavior. Note that if we input a query that does not require a retrieval step, the agent does not execute one:  \\nconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}for s in agent_executor.stream( {\"messages\": [HumanMessage(content=\"Hi! I\\'m bob\")]}, config=config): print(s) print(\"----\")  \\n{\\'agent\\': {\\'messages\\': [AIMessage(content=\\'Hello Bob! How can I assist you today?\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 11, \\'prompt_tokens\\': 67, \\'total_tokens\\': 78}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': None, \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-1cd17562-18aa-4839-b41b-403b17a0fc20-0\\')]}}----  \\nFurther, if we input a query that does require a retrieval step, the agent generates the input to the tool:  \\nquery = \"What is Task Decomposition?\"for s in agent_executor.stream( {\"messages\": [HumanMessage(content=query)]}, config=config): print(s) print(\"----\")  \\nError in LangChainTracer.on_tool_end callback: TracerException(\"Found chain run at ID c54381c0-c5d9-495a-91a0-aca4ae755663, but expected {\\'tool\\'} run.\")``````output{\\'agent\\': {\\'messages\\': [AIMessage(content=\\'\\', additional_kwargs={\\'tool_calls\\': [{\\'id\\': \\'call_rg7zKTE5e0ICxVSslJ1u9LMg\\', \\'function\\': {\\'arguments\\': \\'{\"query\":\"Task Decomposition\"}\\', \\'name\\': \\'blog_post_retriever\\'}, \\'type\\': \\'function\\'}]}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 19, \\'prompt_tokens\\': 91, \\'total_tokens\\': 110}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': None, \\'finish_reason\\': \\'tool_calls\\', \\'logprobs\\': None}, id=\\'run-122bf097-7ff1-49aa-b430-e362b51354ad-0\\', tool_calls=[{\\'name\\': \\'blog_post_retriever\\', \\'args\\': {\\'query\\': \\'Task Decomposition\\'}, \\'id\\': \\'call_rg7zKTE5e0ICxVSslJ1u9LMg\\'}])]}}----{\\'tools\\': {\\'messages\\': [ToolMessage(content=\\'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\\\n\\\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\\\n\\\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\\\n\\\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\', name=\\'blog_post_retriever\\', tool_call_id=\\'call_rg7zKTE5e0ICxVSslJ1u9LMg\\')]}}----{\\'agent\\': {\\'messages\\': [AIMessage(content=\\'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. This approach helps in managing and solving intricate problems by dividing them into more manageable components. By decomposing tasks, agents or models can better understand the steps involved and plan their actions accordingly. Techniques like Chain of Thought (CoT) and Tree of Thoughts are examples of methods that enhance model performance on complex tasks by breaking them down into smaller steps.\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 87, \\'prompt_tokens\\': 659, \\'total_tokens\\': 746}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': None, \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-b9166386-83e5-4b82-9a4b-590e5fa76671-0\\')]}}----  \\nAbove, instead of inserting our query verbatim into the tool, the agent stripped unnecessary words like \"what\" and \"is\".  \\nThis same principle allows the agent to use the context of the conversation when necessary:  \\nquery = \"What according to the blog post are common ways of doing it? redo the search\"for s in agent_executor.stream( {\"messages\": [HumanMessage(content=query)]}, config=config): print(s) print(\"----\")  \\n{\\'agent\\': {\\'messages\\': [AIMessage(content=\\'\\', additional_kwargs={\\'tool_calls\\': [{\\'id\\': \\'call_6kbxTU5CDWLmF9mrvR7bWSkI\\', \\'function\\': {\\'arguments\\': \\'{\"query\":\"Common ways of task decomposition\"}\\', \\'name\\': \\'blog_post_retriever\\'}, \\'type\\': \\'function\\'}]}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 21, \\'prompt_tokens\\': 769, \\'total_tokens\\': 790}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': None, \\'finish_reason\\': \\'tool_calls\\', \\'logprobs\\': None}, id=\\'run-2d2c8327-35cd-484a-b8fd-52436657c2d8-0\\', tool_calls=[{\\'name\\': \\'blog_post_retriever\\', \\'args\\': {\\'query\\': \\'Common ways of task decomposition\\'}, \\'id\\': \\'call_6kbxTU5CDWLmF9mrvR7bWSkI\\'}])]}}----``````outputError in LangChainTracer.on_tool_end callback: TracerException(\"Found chain run at ID 29553415-e0f4-41a9-8921-ba489e377f68, but expected {\\'tool\\'} run.\")``````output{\\'tools\\': {\\'messages\\': [ToolMessage(content=\\'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\\\n\\\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\\\n\\\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\\\n\\\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\', name=\\'blog_post_retriever\\', tool_call_id=\\'call_6kbxTU5CDWLmF9mrvR7bWSkI\\')]}}----{\\'agent\\': {\\'messages\\': [AIMessage(content=\\'Common ways of task decomposition include:\\\\n1. Using LLM with simple prompting like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\"\\\\n2. Using task-specific instructions, for example, \"Write a story outline\" for writing a novel.\\\\n3. Involving human inputs in the task decomposition process.\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 67, \\'prompt_tokens\\': 1339, \\'total_tokens\\': 1406}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': None, \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-9ad14cde-ca75-4238-a868-f865e0fc50dd-0\\')]}}----  \\nNote that the agent was able to infer that \"it\" in our query refers to \"task decomposition\", and generated a reasonable search query as a result-- in this case, \"common ways of task decomposition\".'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Agents\\u200b', 'Header 3': 'Tying it together\\u200b'}, page_content='For convenience, we tie together all of the necessary steps in a single code cell:  \\nimport bs4from langchain.tools.retriever import create_retriever_toolfrom langchain_chroma import Chromafrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)### Construct retriever ###loader = WebBaseLoader( web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(\"post-content\", \"post-title\", \"post-header\") ) ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)splits = text_splitter.split_documents(docs)vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()### Build retriever tool ###tool = create_retriever_tool( retriever, \"blog_post_retriever\", \"Searches and returns excerpts from the Autonomous Agents blog post.\",)tools = [tool]agent_executor = create_react_agent(llm, tools, checkpointer=memory)  \\nAPI Reference:create_retriever_tool | WebBaseLoader | ChatOpenAI | OpenAIEmbeddings | RecursiveCharacterTextSplitter'),\n",
       " Document(metadata={'Header 1': 'How to add chat history', 'Header 2': 'Next steps\\u200b'}, page_content='We\\'ve covered the steps to build a basic conversational Q&A application:  \\nWe used chains to build a predictable application that generates search queries for each user input;We used agents to build an application that \"decides\" when and how to generate search queries.  \\nTo explore different types of retrievers and retrieval strategies, visit the retrievers section of the how-to guides.  \\nFor a detailed walkthrough of LangChain\\'s conversation memory abstractions, visit the How to add message history (memory) LCEL page.  \\nTo learn more about agents, head to the Agents Modules.'),\n",
       " Document(page_content='Edit this page  \\nWas this page helpful?You can also leave detailed feedback on GitHub.  \\nPrevious  \\nHow to use LangChain with different Pydantic versions  \\nNext  \\nHow to get a RAG application to add citations  \\nSetupChainsAgentsNext steps  \\nDependenciesLangSmith  \\nLLMRetrieverPromptAssembling the chainAdding chat historyTying it together  \\nRetrieval toolAgent constructorTying it together  \\nCommunity  \\nTwitter  \\nGitHub  \\nOrganizationPythonJS/TS  \\nMore  \\nHomepageBlogYouTube  \\nCopyright © 2024 LangChain, Inc.')]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Technieques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#os.getenv(\"OPENAI_API_KEY\")\n",
    "#os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x000001B708E57D60>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001B709A244F0>, model='text-embedding-3-large', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embedding_model=OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.013758041895925999,\n",
       " 0.036678798496723175,\n",
       " -0.021104974672198296,\n",
       " -0.01607665605843067,\n",
       " 0.02716689370572567,\n",
       " -0.014205003157258034,\n",
       " -0.0010109017603099346,\n",
       " 0.026873575523495674,\n",
       " -0.029331864789128304,\n",
       " 0.018926037475466728,\n",
       " 0.014665932394564152,\n",
       " -0.03458366543054581,\n",
       " -0.012989826500415802,\n",
       " -0.033913224935531616,\n",
       " 0.006466977763921022,\n",
       " 0.04771316796541214,\n",
       " -0.0010335990227758884,\n",
       " 0.03737717866897583,\n",
       " -0.03436018526554108,\n",
       " -0.022264283150434494,\n",
       " -0.00044041447108611465,\n",
       " 0.001372312312014401,\n",
       " -0.03916502371430397,\n",
       " -0.0459253191947937,\n",
       " 0.02739037573337555,\n",
       " -0.007305031176656485,\n",
       " 0.006683474872261286,\n",
       " 0.044333018362522125,\n",
       " -0.0198199599981308,\n",
       " 0.04517107084393501,\n",
       " 0.012787296436727047,\n",
       " -0.006376188714057207,\n",
       " 0.007109485566616058,\n",
       " -0.009539839811623096,\n",
       " -0.009379212744534016,\n",
       " 0.0023709924425929785,\n",
       " 0.03561726585030556,\n",
       " 0.008981137536466122,\n",
       " -0.052210718393325806,\n",
       " -0.0076332688331604,\n",
       " 0.020350728183984756,\n",
       " -0.0013627095613628626,\n",
       " -0.00791960395872593,\n",
       " -0.008995105512440205,\n",
       " -0.018199723213911057,\n",
       " -0.03181809186935425,\n",
       " -0.015043056569993496,\n",
       " -0.022655373439192772,\n",
       " -0.02192906104028225,\n",
       " -0.031622543931007385,\n",
       " 0.03612009808421135,\n",
       " 0.0008546397439204156,\n",
       " 0.007800879422575235,\n",
       " 0.009134780615568161,\n",
       " -0.01105531957000494,\n",
       " 0.028396038338541985,\n",
       " 0.025323176756501198,\n",
       " 0.021649710834026337,\n",
       " 0.03642738237977028,\n",
       " -0.002217349363490939,\n",
       " 0.006739345379173756,\n",
       " -0.0065193562768399715,\n",
       " -0.031622543931007385,\n",
       " -0.015182732604444027,\n",
       " -0.013380917720496655,\n",
       " -0.027641791850328445,\n",
       " 0.005674319341778755,\n",
       " 0.0060270000249147415,\n",
       " -0.006016524042934179,\n",
       " 0.030896231532096863,\n",
       " 0.015322407707571983,\n",
       " 0.011104206554591656,\n",
       " -0.007514544762670994,\n",
       " -0.002618916565552354,\n",
       " 0.02815859019756317,\n",
       " -0.007423755247145891,\n",
       " 0.036008358001708984,\n",
       " 0.011823534965515137,\n",
       " -0.0218452550470829,\n",
       " 0.027907174080610275,\n",
       " 0.015825239941477776,\n",
       " -0.02377277798950672,\n",
       " 0.03433224931359291,\n",
       " 0.004637228325009346,\n",
       " 0.04207027703523636,\n",
       " -0.012179708108305931,\n",
       " 0.009037007577717304,\n",
       " -0.06168072298169136,\n",
       " -0.011439427733421326,\n",
       " 0.006107313558459282,\n",
       " -0.045226942747831345,\n",
       " 0.032823752611875534,\n",
       " -0.04623260721564293,\n",
       " 0.01579730398952961,\n",
       " 0.013436787761747837,\n",
       " -0.00017983226280193776,\n",
       " 0.04184679314494133,\n",
       " -0.009693482890725136,\n",
       " -0.030086113139986992,\n",
       " 0.046260543167591095,\n",
       " 0.002755100140348077,\n",
       " -0.02113291062414646,\n",
       " 0.005789551418274641,\n",
       " -0.0330193005502224,\n",
       " -0.006341269705444574,\n",
       " -0.014470387250185013,\n",
       " 0.0069802855141460896,\n",
       " 0.02775353193283081,\n",
       " 0.02757195383310318,\n",
       " 0.00778691191226244,\n",
       " 0.00029004501993767917,\n",
       " 0.0005456075887195766,\n",
       " -0.014512289315462112,\n",
       " 0.04883057251572609,\n",
       " -0.011118173599243164,\n",
       " 0.001958949491381645,\n",
       " -0.026217101141810417,\n",
       " -0.04391399398446083,\n",
       " -0.0011208963114768267,\n",
       " -0.031762219965457916,\n",
       " 0.0023168681655079126,\n",
       " -0.03382941707968712,\n",
       " -0.007165355607867241,\n",
       " 0.02032279223203659,\n",
       " -0.005887324456125498,\n",
       " 0.045366618782281876,\n",
       " -0.021468132734298706,\n",
       " -0.0070536150597035885,\n",
       " 0.013576463796198368,\n",
       " 0.000821030349470675,\n",
       " 0.03039339929819107,\n",
       " 0.009686498902738094,\n",
       " -0.002927948720753193,\n",
       " 0.019456803798675537,\n",
       " 0.017263898625969887,\n",
       " -0.008191971108317375,\n",
       " -0.003799174912273884,\n",
       " 0.03371767699718475,\n",
       " 0.02927599474787712,\n",
       " 0.04391399398446083,\n",
       " -0.02044850029051304,\n",
       " -0.017152156680822372,\n",
       " -0.0043054986745119095,\n",
       " -0.017780696973204613,\n",
       " 0.029387734830379486,\n",
       " 0.003554742783308029,\n",
       " -0.04550629481673241,\n",
       " 0.014079295098781586,\n",
       " -0.008848446421325207,\n",
       " 0.01683090440928936,\n",
       " -0.030114049091935158,\n",
       " -0.03530997782945633,\n",
       " -0.00332427816465497,\n",
       " -0.0014482608530670404,\n",
       " 0.025728236883878708,\n",
       " -0.020937364548444748,\n",
       " -0.004804838914424181,\n",
       " 0.030114049091935158,\n",
       " -0.004099477548152208,\n",
       " 0.005255292635411024,\n",
       " 0.014554192312061787,\n",
       " -0.018786361441016197,\n",
       " 0.021272586658596992,\n",
       " 0.05106538161635399,\n",
       " 0.021817320957779884,\n",
       " 0.008052295073866844,\n",
       " -0.00027236732421442866,\n",
       " -0.01780863292515278,\n",
       " -0.05240626633167267,\n",
       " 0.027138959616422653,\n",
       " -0.003346975427120924,\n",
       " 0.016858838498592377,\n",
       " 0.04371844604611397,\n",
       " 0.013304095715284348,\n",
       " -0.05419411137700081,\n",
       " -0.018241627141833305,\n",
       " 0.014484354294836521,\n",
       " 0.011690843850374222,\n",
       " -0.011285784654319286,\n",
       " -0.0025071760173887014,\n",
       " -0.028633488342165947,\n",
       " 0.025490786880254745,\n",
       " 0.016314104199409485,\n",
       " -0.011320703662931919,\n",
       " 0.018325431272387505,\n",
       " -0.0004268834018148482,\n",
       " 0.060228098183870316,\n",
       " 0.00882051046937704,\n",
       " 0.042684849351644516,\n",
       " -0.0330193005502224,\n",
       " 0.011739729903638363,\n",
       " 0.015615726821124554,\n",
       " -0.007165355607867241,\n",
       " 0.0020968790631741285,\n",
       " 0.03159460797905922,\n",
       " -0.0077589768916368484,\n",
       " -0.00232210592366755,\n",
       " 0.031091777607798576,\n",
       " 0.03380148112773895,\n",
       " -0.006498404778540134,\n",
       " -0.015909045934677124,\n",
       " -0.061122018843889236,\n",
       " 0.009770304895937443,\n",
       " -0.004141380079090595,\n",
       " 0.02225031517446041,\n",
       " 0.050339069217443466,\n",
       " -0.0009209856507368386,\n",
       " -0.007430739235132933,\n",
       " 0.01965234987437725,\n",
       " -0.009609677828848362,\n",
       " 0.001689201220870018,\n",
       " -0.029415670782327652,\n",
       " -0.010259169153869152,\n",
       " -0.045673903077840805,\n",
       " 0.05235039442777634,\n",
       " 0.020252954214811325,\n",
       " -0.0631333515048027,\n",
       " 0.0016289660707116127,\n",
       " -0.022012867033481598,\n",
       " 0.0037502883933484554,\n",
       " -0.003263169899582863,\n",
       " 0.032376792281866074,\n",
       " 0.0548645555973053,\n",
       " -0.012773329392075539,\n",
       " -0.03433224931359291,\n",
       " -0.011432443745434284,\n",
       " -0.003617596812546253,\n",
       " -0.040170688182115555,\n",
       " -0.015811271965503693,\n",
       " 0.015098926611244678,\n",
       " 0.01624426618218422,\n",
       " 0.01430277619510889,\n",
       " 0.05106538161635399,\n",
       " 0.00921858660876751,\n",
       " 0.010775968432426453,\n",
       " -0.003956309985369444,\n",
       " 0.026300905272364616,\n",
       " -0.030114049091935158,\n",
       " 0.009700466878712177,\n",
       " 0.04033830016851425,\n",
       " -0.019582511857151985,\n",
       " 0.04131602868437767,\n",
       " -0.0330193005502224,\n",
       " -0.032069504261016846,\n",
       " -0.007123453076928854,\n",
       " 0.05170788988471031,\n",
       " -0.010147429071366787,\n",
       " 0.047601427882909775,\n",
       " -0.010266153141856194,\n",
       " -0.013380917720496655,\n",
       " 0.008464338257908821,\n",
       " 0.01916348561644554,\n",
       " 0.01575540192425251,\n",
       " 0.03195776417851448,\n",
       " -0.02111894264817238,\n",
       " 0.04005894809961319,\n",
       " 0.04542248696088791,\n",
       " -0.0023622626904398203,\n",
       " 0.00731899868696928,\n",
       " -0.005782567895948887,\n",
       " 0.005311162676662207,\n",
       " 0.018562881276011467,\n",
       " 0.025183500722050667,\n",
       " -0.007088534068316221,\n",
       " -0.021719546988606453,\n",
       " -0.0439419262111187,\n",
       " 0.004574374295771122,\n",
       " -0.01866065338253975,\n",
       " 0.007291063666343689,\n",
       " 0.0072980476543307304,\n",
       " 0.03536584973335266,\n",
       " -0.03550552576780319,\n",
       " 0.0677705779671669,\n",
       " 0.04126015678048134,\n",
       " 0.02245982736349106,\n",
       " -0.024624798446893692,\n",
       " -0.017124222591519356,\n",
       " -0.008066263049840927,\n",
       " 0.025476820766925812,\n",
       " -0.03617596626281738,\n",
       " 0.015545888803899288,\n",
       " -0.01812988705933094,\n",
       " 0.03989133611321449,\n",
       " 0.018590815365314484,\n",
       " -0.014721802435815334,\n",
       " -0.03461160138249397,\n",
       " -0.02286488749086857,\n",
       " 0.02229221723973751,\n",
       " -0.05508803576231003,\n",
       " -0.012340335175395012,\n",
       " -0.01197019498795271,\n",
       " -0.010105526074767113,\n",
       " 0.006013032514601946,\n",
       " 0.02600758709013462,\n",
       " 0.003928374964743853,\n",
       " 0.025588560849428177,\n",
       " -0.004553422797471285,\n",
       " -0.012926972471177578,\n",
       " -0.0147916404530406,\n",
       " -0.02783733606338501,\n",
       " -0.016635358333587646,\n",
       " 0.014512289315462112,\n",
       " -0.046567827463150024,\n",
       " -0.03441605344414711,\n",
       " 0.0008205938502214849,\n",
       " 0.03366180881857872,\n",
       " 0.003799174912273884,\n",
       " 0.0026887543499469757,\n",
       " -0.02779543399810791,\n",
       " 0.013408852741122246,\n",
       " 0.00477690389379859,\n",
       " -0.028968708589673042,\n",
       " 0.0065507832914590836,\n",
       " -0.008268792182207108,\n",
       " 0.00014785966777708381,\n",
       " -0.009428099729120731,\n",
       " 0.02287885546684265,\n",
       " -0.0069593340158462524,\n",
       " 0.007800879422575235,\n",
       " -0.018940003588795662,\n",
       " 0.051903434097766876,\n",
       " -0.009965850040316582,\n",
       " -0.0010309801436960697,\n",
       " 0.016356006264686584,\n",
       " -0.013548527844250202,\n",
       " -0.012102886103093624,\n",
       " 0.008219906128942966,\n",
       " 0.0063936482183635235,\n",
       " 0.002264489885419607,\n",
       " -0.023619135841727257,\n",
       " -0.030561009421944618,\n",
       " -0.051763758063316345,\n",
       " 0.0025211437605321407,\n",
       " -0.00879257544875145,\n",
       " 0.018828263506293297,\n",
       " -0.014009458012878895,\n",
       " 0.016174428164958954,\n",
       " 0.0007599222590215504,\n",
       " 0.0003120875626336783,\n",
       " -0.00783579796552658,\n",
       " -0.0002627646317705512,\n",
       " -0.03737717866897583,\n",
       " -0.021984931081533432,\n",
       " -0.02090943045914173,\n",
       " -0.0066241128370165825,\n",
       " -0.012864118441939354,\n",
       " 0.047741103917360306,\n",
       " 0.05564673990011215,\n",
       " 0.013520592823624611,\n",
       " -0.0008472194895148277,\n",
       " 0.0007415898726321757,\n",
       " 0.024499090388417244,\n",
       " -0.00503530353307724,\n",
       " -0.038019683212041855,\n",
       " 0.003921390976756811,\n",
       " -0.027921142056584358,\n",
       " -0.01624426618218422,\n",
       " -0.00289302971214056,\n",
       " -0.03871806338429451,\n",
       " 0.036371514201164246,\n",
       " -0.008184987120330334,\n",
       " -0.0019991062581539154,\n",
       " -0.006281907670199871,\n",
       " -0.013052679598331451,\n",
       " 0.004295023158192635,\n",
       " -0.06128963083028793,\n",
       " -0.006173659116029739,\n",
       " -0.025979653000831604,\n",
       " 0.023870551958680153,\n",
       " 0.012752377428114414,\n",
       " 0.01723596267402172,\n",
       " 0.003886472200974822,\n",
       " -0.013723122887313366,\n",
       " 0.017794664949178696,\n",
       " 0.000578344042878598,\n",
       " -0.01906571164727211,\n",
       " -0.009861093945801258,\n",
       " -0.0004866819945164025,\n",
       " 0.012445091269910336,\n",
       " 0.021733514964580536,\n",
       " 0.018185757100582123,\n",
       " -5.817541023134254e-06,\n",
       " 0.011425459757447243,\n",
       " 0.01951267383992672,\n",
       " -0.0018192740390077233,\n",
       " 0.016006818041205406,\n",
       " 0.008164036087691784,\n",
       " -0.015001153573393822,\n",
       " -0.011956227011978626,\n",
       " 0.04019862413406372,\n",
       " 0.010664228349924088,\n",
       " -0.050339069217443466,\n",
       " 0.005199422128498554,\n",
       " -0.011222930625081062,\n",
       " 0.04958482086658478,\n",
       " 0.0197780579328537,\n",
       " -0.011111189611256123,\n",
       " -0.004951498005539179,\n",
       " 0.02623106725513935,\n",
       " 0.02135639078915119,\n",
       " -0.05137266591191292,\n",
       " -0.016314104199409485,\n",
       " -0.006379680708050728,\n",
       " 0.014582127332687378,\n",
       " -0.004515012260526419,\n",
       " -0.009274456650018692,\n",
       " -0.030337529256939888,\n",
       " -0.026845641434192657,\n",
       " 0.024149902164936066,\n",
       " 0.04494759067893028,\n",
       " -0.0025071760173887014,\n",
       " -0.018744459375739098,\n",
       " -0.04131602868437767,\n",
       " 0.004728017374873161,\n",
       " 0.024052130058407784,\n",
       " 0.019805992022156715,\n",
       " -0.003236980875954032,\n",
       " 0.02922012470662594,\n",
       " -0.021496066823601723,\n",
       " -0.00043801378342323005,\n",
       " 0.023395653814077377,\n",
       " -0.010042672045528889,\n",
       " 0.017599118873476982,\n",
       " 0.01567159593105316,\n",
       " -0.04992004111409187,\n",
       " 0.03223711624741554,\n",
       " -0.003249202389270067,\n",
       " 0.015909045934677124,\n",
       " -0.027013251557946205,\n",
       " -0.04503139853477478,\n",
       " -0.01969425193965435,\n",
       " -0.011341654695570469,\n",
       " 0.012144789099693298,\n",
       " 0.04064558446407318,\n",
       " -0.04511520266532898,\n",
       " -0.02725069969892502,\n",
       " -0.017920373007655144,\n",
       " 0.010489633306860924,\n",
       " -0.014344679191708565,\n",
       " 0.015657629817724228,\n",
       " -0.025099696591496468,\n",
       " -0.03991927206516266,\n",
       " -0.0021632250864058733,\n",
       " -0.0147916404530406,\n",
       " -0.07229606062173843,\n",
       " 0.005880340468138456,\n",
       " -0.03039339929819107,\n",
       " -0.0395561158657074,\n",
       " 0.037041954696178436,\n",
       " -0.04673543944954872,\n",
       " -0.050199393182992935,\n",
       " 0.008778608404099941,\n",
       " -0.06894385069608688,\n",
       " -0.03461160138249397,\n",
       " 0.0039004397112876177,\n",
       " -0.020169150084257126,\n",
       " 0.01616046018898487,\n",
       " 0.039723727852106094,\n",
       " -0.021956996992230415,\n",
       " -0.027460213750600815,\n",
       " 0.0120400320738554,\n",
       " 0.007074566558003426,\n",
       " 0.03427638113498688,\n",
       " 0.010643276385962963,\n",
       " 0.039947208017110825,\n",
       " -0.012605718336999416,\n",
       " 0.005405443720519543,\n",
       " -0.027334505692124367,\n",
       " -0.0015041310107335448,\n",
       " 0.015112894587218761,\n",
       " 0.006040967535227537,\n",
       " -0.008743689395487309,\n",
       " 0.056149572134017944,\n",
       " 0.031985700130462646,\n",
       " 0.017431508749723434,\n",
       " -0.005730189383029938,\n",
       " -0.025588560849428177,\n",
       " -0.0019292684737592936,\n",
       " -0.008212922140955925,\n",
       " -0.013206323608756065,\n",
       " 0.013792960904538631,\n",
       " 0.028479844331741333,\n",
       " 0.01363233383744955,\n",
       " 0.004651195835322142,\n",
       " -0.010238218121230602,\n",
       " 0.016858838498592377,\n",
       " 0.026664061471819878,\n",
       " -0.003022229764610529,\n",
       " -0.025812041014432907,\n",
       " 0.04363464191555977,\n",
       " 0.013373933732509613,\n",
       " 0.032292988151311874,\n",
       " -0.0218033529818058,\n",
       " -0.055032167583703995,\n",
       " -0.002653835341334343,\n",
       " -0.02690151147544384,\n",
       " -0.014624030329287052,\n",
       " 0.004102969076484442,\n",
       " 0.025658398866653442,\n",
       " 0.0218033529818058,\n",
       " -0.023912454023957253,\n",
       " -0.016775032505393028,\n",
       " 0.013897716999053955,\n",
       " 0.011984162032604218,\n",
       " 0.012228594161570072,\n",
       " 0.020574208348989487,\n",
       " -0.018409237265586853,\n",
       " -0.01955457776784897,\n",
       " -0.005244816653430462,\n",
       " 0.02658025734126568,\n",
       " 0.026161231100559235,\n",
       " -0.001238747499883175,\n",
       " 0.0015687310369685292,\n",
       " -0.009497937746345997,\n",
       " 0.010824855417013168,\n",
       " 0.022795049473643303,\n",
       " 0.02922012470662594,\n",
       " 0.021761450916528702,\n",
       " 0.047154463827610016,\n",
       " -0.005876848939806223,\n",
       " -0.009518888778984547,\n",
       " 0.0010938341729342937,\n",
       " 0.026971347630023956,\n",
       " -0.0016176174394786358,\n",
       " -0.018674621358513832,\n",
       " 0.03271201252937317,\n",
       " 0.018870167434215546,\n",
       " -0.02225031517446041,\n",
       " -0.029024578630924225,\n",
       " -0.019722187891602516,\n",
       " -0.012074951082468033,\n",
       " -0.012214627116918564,\n",
       " -0.022627439349889755,\n",
       " -0.025434916839003563,\n",
       " 0.0038131424225866795,\n",
       " -0.015140829607844353,\n",
       " 0.014211987145245075,\n",
       " 0.021007202565670013,\n",
       " -0.01821369118988514,\n",
       " -0.01898190751671791,\n",
       " -0.03223711624741554,\n",
       " 0.034220509231090546,\n",
       " -0.010238218121230602,\n",
       " -0.002341311424970627,\n",
       " -0.028661422431468964,\n",
       " 0.007242177147418261,\n",
       " 0.013304095715284348,\n",
       " -0.04223788529634476,\n",
       " -0.03813142329454422,\n",
       " 0.04748968780040741,\n",
       " 0.016300136223435402,\n",
       " 0.011956227011978626,\n",
       " -0.005049271043390036,\n",
       " 0.013478690758347511,\n",
       " 0.013688203878700733,\n",
       " 0.043522901833057404,\n",
       " -0.017934340983629227,\n",
       " -0.02596568502485752,\n",
       " -0.017487378790974617,\n",
       " -0.008212922140955925,\n",
       " 0.009958866983652115,\n",
       " 0.011474346742033958,\n",
       " -0.009092878550291061,\n",
       " 0.0004600563261192292,\n",
       " 0.006110805086791515,\n",
       " -0.03181809186935425,\n",
       " -0.009637612849473953,\n",
       " 0.03402496501803398,\n",
       " 0.01187940500676632,\n",
       " 0.01741754077374935,\n",
       " 0.010098542086780071,\n",
       " -0.01983392797410488,\n",
       " -0.011362605728209019,\n",
       " 0.026119327172636986,\n",
       " 0.011104206554591656,\n",
       " 0.04234962537884712,\n",
       " 0.013660268858075142,\n",
       " -0.014833543449640274,\n",
       " -0.003896947717294097,\n",
       " 0.021551936864852905,\n",
       " -0.010615341365337372,\n",
       " -0.010650260373950005,\n",
       " -0.001145339454524219,\n",
       " 0.023255979642271996,\n",
       " -0.0005866372957825661,\n",
       " 0.015531920827925205,\n",
       " 0.014065328054130077,\n",
       " -0.014156117103993893,\n",
       " -0.011718778870999813,\n",
       " -0.0027847811579704285,\n",
       " 0.03327071666717529,\n",
       " 0.045366618782281876,\n",
       " 0.011460378766059875,\n",
       " 0.006697442382574081,\n",
       " -0.005147044081240892,\n",
       " 0.0014491338515654206,\n",
       " 0.035198237746953964,\n",
       " 0.037349242717027664,\n",
       " 0.022767115384340286,\n",
       " -0.01250794529914856,\n",
       " -0.0230464655905962,\n",
       " -0.009113829582929611,\n",
       " 0.03796381503343582,\n",
       " 0.026733899489045143,\n",
       " -0.01914951764047146,\n",
       " 0.016942644491791725,\n",
       " 0.03215331211686134,\n",
       " 0.010126477107405663,\n",
       " -0.00415185559540987,\n",
       " 0.0058908164501190186,\n",
       " -0.01794830895960331,\n",
       " -0.025225404649972916,\n",
       " 0.03349419683218002,\n",
       " 0.004116936586797237,\n",
       " 0.008443387225270271,\n",
       " 0.00438581220805645,\n",
       " -0.014582127332687378,\n",
       " 0.04338322579860687,\n",
       " -0.008485289290547371,\n",
       " 0.024135934188961983,\n",
       " 0.031315259635448456,\n",
       " 0.023577231913805008,\n",
       " 0.012228594161570072,\n",
       " -0.01638394221663475,\n",
       " -0.011600054800510406,\n",
       " 0.032823752611875534,\n",
       " 0.006093346048146486,\n",
       " -0.015503985807299614,\n",
       " -0.008240857161581516,\n",
       " 0.010398844256997108,\n",
       " 0.03257233649492264,\n",
       " 0.0059641459956765175,\n",
       " -0.015531920827925205,\n",
       " 0.005188946612179279,\n",
       " 0.016956612467765808,\n",
       " -0.01410024706274271,\n",
       " 0.0010912151774391532,\n",
       " -0.01799021102488041,\n",
       " 0.018046081066131592,\n",
       " 0.0002939733676612377,\n",
       " 0.03874599561095238,\n",
       " -0.008680835366249084,\n",
       " -0.014526257291436195,\n",
       " 0.06285399943590164,\n",
       " -0.02103513851761818,\n",
       " -0.013702170923352242,\n",
       " 0.016453780233860016,\n",
       " -0.008471322245895863,\n",
       " 0.018325431272387505,\n",
       " -0.022487763315439224,\n",
       " 0.019275225698947906,\n",
       " -0.023074401542544365,\n",
       " 0.02815859019756317,\n",
       " 0.012047016061842442,\n",
       " -0.007360901217907667,\n",
       " -0.016677260398864746,\n",
       " -0.01250794529914856,\n",
       " 0.0021789385937154293,\n",
       " -0.0023151221685111523,\n",
       " 0.021873190999031067,\n",
       " -0.009972834028303623,\n",
       " 0.021859223023056984,\n",
       " 0.011383557692170143,\n",
       " -0.03419257327914238,\n",
       " 0.035114433616399765,\n",
       " -0.01329012867063284,\n",
       " -0.011404508724808693,\n",
       " -0.024457188323140144,\n",
       " 0.0022190953604876995,\n",
       " 0.014218971133232117,\n",
       " 0.0010056640021502972,\n",
       " -0.013660268858075142,\n",
       " -0.039192959666252136,\n",
       " -0.006323810666799545,\n",
       " 0.010636293329298496,\n",
       " 0.014358646236360073,\n",
       " -0.0017302308697253466,\n",
       " 0.029192190617322922,\n",
       " 0.040254492312669754,\n",
       " 0.013695187866687775,\n",
       " -0.011327686719596386,\n",
       " -0.02174748294055462,\n",
       " 0.022795049473643303,\n",
       " -0.00923953764140606,\n",
       " -0.009546823799610138,\n",
       " -0.007954522967338562,\n",
       " -0.019708219915628433,\n",
       " 0.04184679314494133,\n",
       " -0.029164254665374756,\n",
       " -0.0050737145356833935,\n",
       " 0.005817486438900232,\n",
       " -0.008652900345623493,\n",
       " 0.031035907566547394,\n",
       " 0.005541627295315266,\n",
       " 0.011404508724808693,\n",
       " 0.012347318232059479,\n",
       " -0.007312015164643526,\n",
       " 0.0018576848087832332,\n",
       " 0.02891283854842186,\n",
       " -0.012047016061842442,\n",
       " -0.03625977411866188,\n",
       " 0.01593698002398014,\n",
       " -0.02712499164044857,\n",
       " 0.02178938500583172,\n",
       " -0.0019083172082901,\n",
       " -0.0006377061945386231,\n",
       " 0.012179708108305931,\n",
       " 0.004378828685730696,\n",
       " -0.007584382314234972,\n",
       " -0.010985481552779675,\n",
       " 0.006585702300071716,\n",
       " -0.009679515846073627,\n",
       " 0.0004897373728454113,\n",
       " 0.050339069217443466,\n",
       " -0.021496066823601723,\n",
       " 0.010545504279434681,\n",
       " -0.00037625100230798125,\n",
       " 0.016719162464141846,\n",
       " 0.012745393440127373,\n",
       " -0.0012457312550395727,\n",
       " -0.004193758126348257,\n",
       " 0.016356006264686584,\n",
       " 0.013304095715284348,\n",
       " -0.013325047679245472,\n",
       " 0.019680283963680267,\n",
       " 0.02529524266719818,\n",
       " -0.0021510033402591944,\n",
       " -0.02537904679775238,\n",
       " -0.0003147064708173275,\n",
       " 0.006788231898099184,\n",
       " 0.0014665932394564152,\n",
       " -0.028745228424668312,\n",
       " -0.01230541616678238,\n",
       " 0.016998514533042908,\n",
       " 0.011760680936276913,\n",
       " -0.03648325428366661,\n",
       " 0.001646425575017929,\n",
       " 0.012919988483190536,\n",
       " -0.02596568502485752,\n",
       " 0.0003856354742310941,\n",
       " 0.032823752611875534,\n",
       " -0.02980676293373108,\n",
       " -0.022445861250162125,\n",
       " 0.013366949744522572,\n",
       " -0.010461698286235332,\n",
       " -0.014344679191708565,\n",
       " 0.016006818041205406,\n",
       " 0.022906789556145668,\n",
       " -0.020113278180360794,\n",
       " 0.014232938177883625,\n",
       " 0.011872421950101852,\n",
       " -0.0035320455208420753,\n",
       " 0.01284316647797823,\n",
       " -0.02028089016675949,\n",
       " -0.03142699971795082,\n",
       " 0.01280126441270113,\n",
       " 0.021021170541644096,\n",
       " -0.012284464202821255,\n",
       " -0.0031444458290934563,\n",
       " 0.012892053462564945,\n",
       " 5.518821126315743e-05,\n",
       " 0.03880186751484871,\n",
       " -0.013206323608756065,\n",
       " 0.03620390221476555,\n",
       " 0.024918118491768837,\n",
       " 0.006924415472894907,\n",
       " 0.01378597691655159,\n",
       " 0.00040178545168600976,\n",
       " -0.013974539004266262,\n",
       " -0.01691470853984356,\n",
       " -4.211408850096632e-06,\n",
       " 0.0047664279118180275,\n",
       " 0.007870716974139214,\n",
       " 0.011460378766059875,\n",
       " -0.015476050786674023,\n",
       " -0.018004179000854492,\n",
       " 0.02058817632496357,\n",
       " -0.005485757254064083,\n",
       " -0.01906571164727211,\n",
       " -0.021719546988606453,\n",
       " -0.008478306233882904,\n",
       " -0.02282298542559147,\n",
       " 0.02009931206703186,\n",
       " 0.016090624034404755,\n",
       " -0.020616110414266586,\n",
       " -0.031846024096012115,\n",
       " 0.00796849001199007,\n",
       " -0.009651580825448036,\n",
       " 0.01253588031977415,\n",
       " 0.009532855823636055,\n",
       " -0.02980676293373108,\n",
       " -0.013778992928564548,\n",
       " 0.013834862969815731,\n",
       " 0.019931700080633163,\n",
       " 0.009162715636193752,\n",
       " 0.004120428580790758,\n",
       " -0.010035688057541847,\n",
       " 0.00279176514595747,\n",
       " 0.0038096506614238024,\n",
       " -0.010342974215745926,\n",
       " 0.020211052149534225,\n",
       " 0.00843640323728323,\n",
       " 0.024582896381616592,\n",
       " 0.0351703017950058,\n",
       " -0.013820895925164223,\n",
       " 0.020699916407465935,\n",
       " -0.004975941497832537,\n",
       " 0.007277096156030893,\n",
       " 0.028410006314516068,\n",
       " 0.0020078360103070736,\n",
       " 0.0025124140083789825,\n",
       " -0.004665163345634937,\n",
       " 0.009302391670644283,\n",
       " 0.026245035231113434,\n",
       " 0.019317127764225006,\n",
       " 0.02694341354072094,\n",
       " -0.007661203853785992,\n",
       " 0.006767280399799347,\n",
       " -0.024946052581071854,\n",
       " -0.015517953783273697,\n",
       " 0.024024194106459618,\n",
       " 0.002786527154967189,\n",
       " 0.004658179357647896,\n",
       " 0.009665547870099545,\n",
       " -0.007779927924275398,\n",
       " -0.0007119087968021631,\n",
       " 0.016844870522618294,\n",
       " 0.016411876305937767,\n",
       " 0.009609677828848362,\n",
       " -0.01983392797410488,\n",
       " 0.010762001387774944,\n",
       " -0.010964530520141125,\n",
       " -0.03528204187750816,\n",
       " 0.0263148732483387,\n",
       " -0.00441025523468852,\n",
       " 0.035561393946409225,\n",
       " -0.0018419713014736772,\n",
       " -0.004874676465988159,\n",
       " 0.04698685556650162,\n",
       " -0.010042672045528889,\n",
       " -0.015909045934677124,\n",
       " 0.005559086799621582,\n",
       " -0.032069504261016846,\n",
       " -0.008876381441950798,\n",
       " -0.0032649158965796232,\n",
       " -0.0024757490027695894,\n",
       " -0.01181655190885067,\n",
       " -0.03371767699718475,\n",
       " -5.832545502926223e-05,\n",
       " -0.011921308003365993,\n",
       " 0.026119327172636986,\n",
       " 0.022040801122784615,\n",
       " -0.02292075753211975,\n",
       " -0.0054717897437512875,\n",
       " 0.017571184784173965,\n",
       " 0.020602144300937653,\n",
       " 0.013569479808211327,\n",
       " 0.007388836704194546,\n",
       " 0.013702170923352242,\n",
       " 0.01825559511780739,\n",
       " 0.028633488342165947,\n",
       " 0.006547291297465563,\n",
       " -0.032376792281866074,\n",
       " 0.036902278661727905,\n",
       " 0.020252954214811325,\n",
       " -0.02972295694053173,\n",
       " -0.01282221544533968,\n",
       " -0.014875445514917374,\n",
       " 0.0007210750482045114,\n",
       " -0.03877393156290054,\n",
       " -0.002638121834024787,\n",
       " 0.001614998560398817,\n",
       " -0.003914407454431057,\n",
       " 0.026119327172636986,\n",
       " 0.01914951764047146,\n",
       " 0.03880186751484871,\n",
       " -0.008108166046440601,\n",
       " -0.018367335200309753,\n",
       " -0.02438735030591488,\n",
       " -0.028521746397018433,\n",
       " 0.02162177488207817,\n",
       " 0.03829903528094292,\n",
       " -0.008443387225270271,\n",
       " 0.020616110414266586,\n",
       " 0.017026448622345924,\n",
       " -0.001924030715599656,\n",
       " -0.018241627141833305,\n",
       " -0.016956612467765808,\n",
       " 0.021873190999031067,\n",
       " -0.02550475485622883,\n",
       " 0.0198199599981308,\n",
       " -0.005436870735138655,\n",
       " 0.04226582124829292,\n",
       " -0.057490456849336624,\n",
       " 0.03366180881857872,\n",
       " 0.02533714473247528,\n",
       " -0.020755786448717117,\n",
       " 0.006840609945356846,\n",
       " -0.015992850065231323,\n",
       " -0.0175432488322258,\n",
       " 0.004930546972900629,\n",
       " -0.03827109932899475,\n",
       " -0.014889413490891457,\n",
       " -0.005772091913968325,\n",
       " 0.04505933076143265,\n",
       " -0.015294472686946392,\n",
       " 0.03321484476327896,\n",
       " 0.0057336813770234585,\n",
       " 0.007724057883024216,\n",
       " 0.0048607089556753635,\n",
       " -0.0032265051268041134,\n",
       " 0.00814308412373066,\n",
       " 0.04123222082853317,\n",
       " -0.010678195394575596,\n",
       " 0.015057024545967579,\n",
       " 0.0011174044338986278,\n",
       " -0.005824470426887274,\n",
       " -0.00369791011326015,\n",
       " 0.04785284399986267,\n",
       " 0.012878085486590862,\n",
       " -0.007039647549390793,\n",
       " 0.04341116175055504,\n",
       " 0.0396399199962616,\n",
       " 0.004574374295771122,\n",
       " 0.02568633295595646,\n",
       " -0.025448884814977646,\n",
       " -0.026999283581972122,\n",
       " -0.007507560774683952,\n",
       " 0.010817871429026127,\n",
       " 0.02286488749086857,\n",
       " -0.009979818016290665,\n",
       " 0.0038340939208865166,\n",
       " 0.007325982674956322,\n",
       " 0.013765024952590466,\n",
       " 0.015434147790074348,\n",
       " -0.004822298418730497,\n",
       " -0.022222379222512245,\n",
       " 0.02806081809103489,\n",
       " -0.038606323301792145,\n",
       " -0.008359581232070923,\n",
       " 0.03366180881857872,\n",
       " 0.0184930432587862,\n",
       " -0.002583997556939721,\n",
       " 0.0013513609301298857,\n",
       " 0.002009582007303834,\n",
       " -0.04992004111409187,\n",
       " 0.009092878550291061,\n",
       " -0.0015373040223494172,\n",
       " -0.002880808198824525,\n",
       " -0.030505139380693436,\n",
       " -0.0013967554550617933,\n",
       " 0.010391861200332642,\n",
       " 0.015685563907027245,\n",
       " 0.006770772393792868,\n",
       " 0.008136101067066193,\n",
       " -0.0024408302269876003,\n",
       " -0.03190189599990845,\n",
       " -0.015392245724797249,\n",
       " -0.023702939972281456,\n",
       " -0.0016298390692099929,\n",
       " -0.018632717430591583,\n",
       " -0.0062574646435678005,\n",
       " 0.008897332474589348,\n",
       " -0.002035771030932665,\n",
       " 0.0007481371867470443,\n",
       " -0.013681219890713692,\n",
       " 0.018828263506293297,\n",
       " -0.013702170923352242,\n",
       " -0.002943662228062749,\n",
       " -0.01719406060874462,\n",
       " -0.013897716999053955,\n",
       " 0.013660268858075142,\n",
       " 0.03039339929819107,\n",
       " -0.019093647599220276,\n",
       " 0.02739037573337555,\n",
       " 0.0004253556835465133,\n",
       " -0.0016804714687168598,\n",
       " 0.02891283854842186,\n",
       " -0.011174043640494347,\n",
       " -0.0033836401998996735,\n",
       " 0.004225185140967369,\n",
       " -0.006316826678812504,\n",
       " 0.025798073038458824,\n",
       " 0.033550065010786057,\n",
       " 0.013429803773760796,\n",
       " 0.01020329911261797,\n",
       " 0.013911684975028038,\n",
       " -0.011711794883012772,\n",
       " -0.02144019678235054,\n",
       " -0.011425459757447243,\n",
       " 0.009490953758358955,\n",
       " 0.006774263922125101,\n",
       " -0.008066263049840927,\n",
       " -0.018157821148633957,\n",
       " -0.011942259036004543,\n",
       " 0.0032125376164913177,\n",
       " 0.010775968432426453,\n",
       " 0.0015373040223494172,\n",
       " -0.0004015672020614147,\n",
       " -0.02345152385532856,\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"This is a tutorial of OPENAI embeddings and I am Karan Mehta\"\n",
    "query_results=embedding_model.embed_query(text)\n",
    "query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_1024=OpenAIEmbeddings(model=\"text-embedding-3-large\",dimensions=1024)\n",
    "query_1024=embeddings_1024.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\nâ€¦\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between usâ€”however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendshipâ€”exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our heartsâ€”for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating embeddings from sppech.txt file\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader('D:\\Langchain_Project\\Data Ingestion\\speech.txt')\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='Just because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\nâ€¦'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='It will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between usâ€”however hard it may be for them, for the time being, to believe that this is spoken from our hearts.'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='We have borne with their present government through all these bitter months because of that friendshipâ€”exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='here and there and without countenance except from a lawless and malignant few.'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='It is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our heartsâ€”for'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='we have always carried nearest our heartsâ€”for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='To such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "final_documents=text_splitter.split_documents(docs)\n",
    "final_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x0000020F6C38E2C0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x0000020F6D8C1240>, model='text-embedding-3-large', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embedding_model=OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x20f6ad74a00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "db=Chroma.from_documents(final_documents, embedding_model)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='It will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='Just because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\nâ€¦'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between usâ€”however hard it may be for them, for the time being, to believe that this is spoken from our hearts.'),\n",
       " Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='We have borne with their present government through all these bitter months because of that friendshipâ€”exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"It will be all the easier for us to conduct ourselves as belligerent\"\n",
    "retrieved_results=db.similarity_search(query)\n",
    "retrieved_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='It will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early'),\n",
       "  0.9492225050926208),\n",
       " (Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='Just because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\nâ€¦'),\n",
       "  1.119370698928833),\n",
       " (Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between usâ€”however hard it may be for them, for the time being, to believe that this is spoken from our hearts.'),\n",
       "  1.2573343515396118),\n",
       " (Document(metadata={'source': 'D:\\\\Langchain_Project\\\\Data Ingestion\\\\speech.txt'}, page_content='We have borne with their present government through all these bitter months because of that friendshipâ€”exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the'),\n",
       "  1.2691917419433594)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Olama Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings=(\n",
    "    OllamaEmbeddings(model=\"gemma:2b\")   #By-Default it uses llama2\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
